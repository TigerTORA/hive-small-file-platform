#!/usr/bin/env python3
"""
æ¼”ç¤ºæ•°æ®ç”Ÿæˆå™¨
ç”Ÿæˆç”¨äºæ¼”ç¤ºçš„æ¨¡æ‹Ÿ Hive é›†ç¾¤æ•°æ®ã€è¡¨ä¿¡æ¯å’Œä»»åŠ¡è®°å½•
"""

import argparse
import json
import random
import os
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.models.base import Base
from app.models.cluster import Cluster
from app.models.table_metric import TableMetric
from app.models.scan_task import ScanTask
from app.models.merge_task import MergeTask

# æ¼”ç¤ºæ•°æ®é¢„è®¾
DEMO_PRESETS = {
    'comprehensive': {
        'clusters': 3,
        'databases_per_cluster': 8,
        'tables_per_database': 25,
        'scan_tasks': 50,
        'merge_tasks': 30,
        'description': 'ç»¼åˆæ¼”ç¤º - å¤šé›†ç¾¤ã€å¤šæ•°æ®åº“ã€ä¸°å¯Œçš„è¡¨å’Œä»»åŠ¡æ•°æ®'
    },
    'performance': {
        'clusters': 2,
        'databases_per_cluster': 12,
        'tables_per_database': 40,
        'scan_tasks': 80,
        'merge_tasks': 50,
        'description': 'æ€§èƒ½æ¼”ç¤º - å¤§é‡æ•°æ®å±•ç¤ºç³»ç»Ÿå¤„ç†èƒ½åŠ›'
    },
    'small_scale': {
        'clusters': 1,
        'databases_per_cluster': 4,
        'tables_per_database': 15,
        'scan_tasks': 20,
        'merge_tasks': 10,
        'description': 'å°è§„æ¨¡æ¼”ç¤º - é€‚åˆå¿«é€Ÿå±•ç¤ºå’Œæµ‹è¯•'
    },
    'large_scale': {
        'clusters': 5,
        'databases_per_cluster': 15,
        'tables_per_database': 60,
        'scan_tasks': 150,
        'merge_tasks': 100,
        'description': 'å¤§è§„æ¨¡æ¼”ç¤º - å±•ç¤ºä¼ä¸šçº§è§„æ¨¡çš„æ•°æ®æ²»ç†'
    }
}

class DemoDataGenerator:
    def __init__(self, database_url: str, preset: str = 'comprehensive'):
        self.engine = create_engine(database_url)
        self.SessionLocal = sessionmaker(bind=self.engine)
        self.preset_config = DEMO_PRESETS[preset]
        self.preset_name = preset

        print(f"ğŸ¯ ä½¿ç”¨é¢„è®¾: {preset}")
        print(f"ğŸ“ æè¿°: {self.preset_config['description']}")

    def create_tables(self):
        """åˆ›å»ºæ•°æ®åº“è¡¨"""
        print("ğŸ“Š åˆ›å»ºæ•°æ®åº“è¡¨...")
        Base.metadata.create_all(bind=self.engine)

    def generate_clusters(self, session) -> List[Cluster]:
        """ç”Ÿæˆé›†ç¾¤æ•°æ®"""
        clusters = []
        cluster_names = [
            'production-cluster', 'staging-cluster', 'development-cluster',
            'analytics-cluster', 'ml-cluster'
        ]

        environments = ['production', 'staging', 'development', 'testing', 'analytics']

        for i in range(self.preset_config['clusters']):
            cluster = Cluster(
                name=cluster_names[i % len(cluster_names)],
                description=f"æ¼”ç¤ºé›†ç¾¤ {i+1} - {environments[i % len(environments)]}ç¯å¢ƒ",
                hive_metastore_url=f"mysql://hive:hive@metastore-{i+1}:3306/hive_metastore",
                hdfs_namenode_url=f"http://namenode-{i+1}:9870/webhdfs/v1",
                hdfs_user="hdfs",
                status="active",
                environment=environments[i % len(environments)],
                created_time=datetime.utcnow() - timedelta(days=random.randint(30, 365))
            )
            session.add(cluster)
            clusters.append(cluster)

        session.commit()
        print(f"âœ… ç”Ÿæˆäº† {len(clusters)} ä¸ªé›†ç¾¤")
        return clusters

    def generate_table_metrics(self, session, clusters: List[Cluster]):
        """ç”Ÿæˆè¡¨æŒ‡æ ‡æ•°æ®"""
        print("ğŸ“ˆ ç”Ÿæˆè¡¨æŒ‡æ ‡æ•°æ®...")

        # æ•°æ®åº“åç§°æ¨¡æ¿
        database_templates = [
            'user_data', 'product_catalog', 'order_management', 'analytics',
            'log_data', 'sensor_data', 'financial_data', 'marketing_data',
            'inventory', 'customer_service', 'recommendation', 'fraud_detection'
        ]

        # è¡¨åç§°æ¨¡æ¿
        table_templates = [
            'user_profile', 'user_behavior', 'user_sessions',
            'product_info', 'product_reviews', 'product_sales',
            'order_details', 'order_items', 'order_status',
            'click_stream', 'page_views', 'events',
            'transaction_log', 'error_log', 'access_log',
            'daily_summary', 'hourly_stats', 'real_time_metrics'
        ]

        total_tables = 0

        for cluster in clusters:
            for db_idx in range(self.preset_config['databases_per_cluster']):
                database_name = f"{database_templates[db_idx % len(database_templates)]}_{db_idx+1}"

                for table_idx in range(self.preset_config['tables_per_database']):
                    table_name = f"{table_templates[table_idx % len(table_templates)]}_{table_idx+1}"

                    # ç”ŸæˆçœŸå®çš„å°æ–‡ä»¶åˆ†å¸ƒæ•°æ®
                    total_files = random.randint(10, 10000)
                    small_file_ratio = random.uniform(0.1, 0.9)
                    small_files = int(total_files * small_file_ratio)

                    # è®¡ç®—æ–‡ä»¶å¤§å°
                    avg_file_size = random.randint(1024, 256*1024*1024)  # 1KB - 256MB
                    total_size = total_files * avg_file_size

                    # åˆ†åŒºä¿¡æ¯
                    is_partitioned = random.choice([True, False])
                    partition_count = random.randint(1, 100) if is_partitioned else 1

                    table_metric = TableMetric(
                        cluster_id=cluster.id,
                        database_name=database_name,
                        table_name=table_name,
                        table_type=random.choice(['MANAGED_TABLE', 'EXTERNAL_TABLE']),
                        file_count=total_files,
                        small_file_count=small_files,
                        total_size=total_size,
                        small_file_ratio=small_file_ratio * 100,
                        avg_file_size=avg_file_size,
                        is_partitioned=is_partitioned,
                        partition_count=partition_count,
                        last_scan_time=datetime.utcnow() - timedelta(
                            hours=random.randint(1, 72)
                        ),
                        created_time=datetime.utcnow() - timedelta(
                            days=random.randint(1, 30)
                        )
                    )

                    session.add(table_metric)
                    total_tables += 1

        session.commit()
        print(f"âœ… ç”Ÿæˆäº† {total_tables} ä¸ªè¡¨çš„æŒ‡æ ‡æ•°æ®")

    def generate_scan_tasks(self, session, clusters: List[Cluster]):
        """ç”Ÿæˆæ‰«æä»»åŠ¡æ•°æ®"""
        print("ğŸ” ç”Ÿæˆæ‰«æä»»åŠ¡æ•°æ®...")

        task_types = ['cluster', 'database', 'table']
        statuses = ['completed', 'failed', 'running', 'pending']
        status_weights = [0.7, 0.1, 0.1, 0.1]  # å¤§éƒ¨åˆ†ä»»åŠ¡æ˜¯å®Œæˆçš„

        for i in range(self.preset_config['scan_tasks']):
            cluster = random.choice(clusters)
            task_type = random.choice(task_types)
            status = random.choices(statuses, weights=status_weights)[0]

            start_time = datetime.utcnow() - timedelta(
                hours=random.randint(1, 720)  # æœ€è¿‘30å¤©
            )

            # æ ¹æ®çŠ¶æ€è®¾ç½®ç»“æŸæ—¶é—´å’ŒæŒç»­æ—¶é—´
            if status == 'completed':
                duration = random.randint(30, 3600)  # 30ç§’åˆ°1å°æ—¶
                end_time = start_time + timedelta(seconds=duration)
            elif status == 'failed':
                duration = random.randint(10, 1800)  # 10ç§’åˆ°30åˆ†é’Ÿ
                end_time = start_time + timedelta(seconds=duration)
            else:
                duration = None
                end_time = None

            # ç”Ÿæˆä»»åŠ¡ç»“æœæ•°æ®
            if status == 'completed':
                total_tables = random.randint(10, 1000)
                total_files = random.randint(total_tables * 5, total_tables * 50)
                small_files = int(total_files * random.uniform(0.3, 0.8))
            else:
                total_tables = random.randint(0, 100) if status == 'failed' else 0
                total_files = random.randint(0, total_tables * 10) if total_tables > 0 else 0
                small_files = int(total_files * random.uniform(0.3, 0.8)) if total_files > 0 else 0

            scan_task = ScanTask(
                cluster_id=cluster.id,
                task_type=task_type,
                task_name=f"{task_type.title()} æ‰«æä»»åŠ¡ #{i+1}",
                status=status,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                total_tables_scanned=total_tables,
                total_files_found=total_files,
                total_small_files=small_files,
                error_message="æ¨¡æ‹Ÿæ‰«æé”™è¯¯ï¼šè¿æ¥è¶…æ—¶" if status == 'failed' else None
            )

            session.add(scan_task)

        session.commit()
        print(f"âœ… ç”Ÿæˆäº† {self.preset_config['scan_tasks']} ä¸ªæ‰«æä»»åŠ¡")

    def generate_merge_tasks(self, session, clusters: List[Cluster]):
        """ç”Ÿæˆåˆå¹¶ä»»åŠ¡æ•°æ®"""
        print("ğŸ”§ ç”Ÿæˆåˆå¹¶ä»»åŠ¡æ•°æ®...")

        merge_strategies = ['safe_merge', 'concatenate', 'insert_overwrite']
        statuses = ['completed', 'failed', 'running', 'pending']
        status_weights = [0.6, 0.15, 0.15, 0.1]

        # è·å–ä¸€äº›è¡¨åç”¨äºä»»åŠ¡
        table_names = [
            'user_behavior_logs', 'product_sales_daily', 'click_stream_raw',
            'sensor_data_hourly', 'transaction_details', 'order_items_fact',
            'customer_interactions', 'inventory_snapshots', 'financial_records'
        ]

        database_names = [
            'analytics_db', 'sales_db', 'user_db', 'product_db', 'log_db'
        ]

        for i in range(self.preset_config['merge_tasks']):
            cluster = random.choice(clusters)
            strategy = random.choice(merge_strategies)
            status = random.choices(statuses, weights=status_weights)[0]

            database_name = random.choice(database_names)
            table_name = random.choice(table_names)

            created_time = datetime.utcnow() - timedelta(
                hours=random.randint(1, 720)
            )

            # ç”Ÿæˆåˆå¹¶å‰åçš„æ–‡ä»¶æ•°æ®
            files_before = random.randint(100, 5000)
            if status == 'completed':
                reduction_ratio = random.uniform(0.6, 0.9)  # 60%-90%çš„æ–‡ä»¶å‡å°‘
                files_after = int(files_before * (1 - reduction_ratio))
            else:
                files_after = files_before if status in ['failed', 'pending'] else None

            merge_task = MergeTask(
                cluster_id=cluster.id,
                task_name=f"{table_name} å°æ–‡ä»¶åˆå¹¶ #{i+1}",
                database_name=database_name,
                table_name=table_name,
                merge_strategy=strategy,
                status=status,
                files_before=files_before,
                files_after=files_after,
                target_file_size=random.choice([64, 128, 256, 512]) * 1024 * 1024,  # MB
                partition_filter=f"dt>='{(datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')}'" if random.choice([True, False]) else None,
                created_time=created_time
            )

            session.add(merge_task)

        session.commit()
        print(f"âœ… ç”Ÿæˆäº† {self.preset_config['merge_tasks']} ä¸ªåˆå¹¶ä»»åŠ¡")

    def generate_summary_data(self, session):
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡æ•°æ®"""
        print("ğŸ“Š ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡æ•°æ®...")

        # è®¡ç®—å®é™…ç»Ÿè®¡æ•°æ®
        total_clusters = session.query(Cluster).count()
        total_tables = session.query(TableMetric).count()
        total_small_files = session.query(TableMetric).with_entities(
            session.query(TableMetric.small_file_count).label('sum')
        ).scalar() or 0

        # è®¡ç®—é—®é¢˜è¡¨æ•°é‡ï¼ˆå°æ–‡ä»¶æ¯”ä¾‹ > 50% çš„è¡¨ï¼‰
        problem_tables = session.query(TableMetric).filter(
            TableMetric.small_file_ratio > 50
        ).count()

        summary_data = {
            'generated_at': datetime.utcnow().isoformat(),
            'preset': self.preset_name,
            'preset_description': self.preset_config['description'],
            'statistics': {
                'total_clusters': total_clusters,
                'total_tables': total_tables,
                'total_small_files': int(total_small_files),
                'problem_tables': problem_tables,
                'scan_tasks': self.preset_config['scan_tasks'],
                'merge_tasks': self.preset_config['merge_tasks']
            },
            'features_enabled': {
                'demo_mode': True,
                'advanced_charts': True,
                'smart_recommendations': True,
                'performance_monitoring': True,
                'fullscreen_mode': True
            }
        }

        # ä¿å­˜æ±‡æ€»æ•°æ®åˆ°æ–‡ä»¶
        os.makedirs('demo-data', exist_ok=True)
        with open('demo-data/summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“ˆ æ±‡æ€»æ•°æ®:")
        print(f"   â€¢ é›†ç¾¤æ•°é‡: {total_clusters}")
        print(f"   â€¢ è¡¨æ€»æ•°: {total_tables}")
        print(f"   â€¢ å°æ–‡ä»¶æ€»æ•°: {total_small_files:,}")
        print(f"   â€¢ é—®é¢˜è¡¨æ•°é‡: {problem_tables}")

    def generate_all(self):
        """ç”Ÿæˆæ‰€æœ‰æ¼”ç¤ºæ•°æ®"""
        print(f"ğŸš€ å¼€å§‹ç”Ÿæˆæ¼”ç¤ºæ•°æ®...")
        print(f"ğŸ“‹ é¢„è®¾é…ç½®: {self.preset_config}")

        session = self.SessionLocal()

        try:
            # æ¸…ç†ç°æœ‰æ•°æ®ï¼ˆå¯é€‰ï¼‰
            if os.getenv('CLEAR_EXISTING_DATA', 'false').lower() == 'true':
                print("ğŸ§¹ æ¸…ç†ç°æœ‰æ•°æ®...")
                session.query(MergeTask).delete()
                session.query(ScanTask).delete()
                session.query(TableMetric).delete()
                session.query(Cluster).delete()
                session.commit()

            # åˆ›å»ºè¡¨
            self.create_tables()

            # ç”Ÿæˆæ•°æ®
            clusters = self.generate_clusters(session)
            self.generate_table_metrics(session, clusters)
            self.generate_scan_tasks(session, clusters)
            self.generate_merge_tasks(session, clusters)
            self.generate_summary_data(session)

            print("ğŸ‰ æ¼”ç¤ºæ•°æ®ç”Ÿæˆå®Œæˆ!")

        except Exception as e:
            session.rollback()
            print(f"âŒ ç”Ÿæˆæ•°æ®æ—¶å‡ºé”™: {e}")
            raise
        finally:
            session.close()

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆ Hive å°æ–‡ä»¶æ²»ç†å¹³å°æ¼”ç¤ºæ•°æ®')
    parser.add_argument(
        '--preset',
        choices=list(DEMO_PRESETS.keys()),
        default='comprehensive',
        help='æ¼”ç¤ºæ•°æ®é¢„è®¾'
    )
    parser.add_argument(
        '--database-url',
        default=os.getenv('DATABASE_URL', 'postgresql://postgres:postgres@localhost:5432/hive_demo_db'),
        help='æ•°æ®åº“è¿æ¥URL'
    )
    parser.add_argument(
        '--clear',
        action='store_true',
        help='æ¸…ç†ç°æœ‰æ•°æ®'
    )

    args = parser.parse_args()

    if args.clear:
        os.environ['CLEAR_EXISTING_DATA'] = 'true'

    print("ğŸ¯ Hive å°æ–‡ä»¶æ²»ç†å¹³å° - æ¼”ç¤ºæ•°æ®ç”Ÿæˆå™¨")
    print("=" * 50)

    # æ˜¾ç¤ºå¯ç”¨é¢„è®¾
    print("ğŸ“‹ å¯ç”¨çš„æ•°æ®é¢„è®¾:")
    for name, config in DEMO_PRESETS.items():
        marker = "ğŸ‘‰" if name == args.preset else "  "
        print(f"{marker} {name}: {config['description']}")
    print()

    generator = DemoDataGenerator(args.database_url, args.preset)
    generator.generate_all()

if __name__ == '__main__':
    main()