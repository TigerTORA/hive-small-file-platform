#!/usr/bin/env python3
"""
çœŸå®æ•°æ®æ‰«ææµ‹è¯•
- ä½¿ç”¨çœŸå®çš„MySQL Hive MetaStore
- æ™ºèƒ½é€‰æ‹©HDFSæ‰«æå™¨ï¼ˆçœŸå®/Mockï¼‰
- æ¼”ç¤ºå®Œæ•´çš„è¡¨æ‰«ææµç¨‹
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "backend"))

import logging

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.config.database import Base
from app.models.cluster import Cluster
from app.models.partition_metric import PartitionMetric
from app.models.table_metric import TableMetric
from app.monitor.hybrid_table_scanner import HybridTableScanner

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def setup_test_database():
    """è®¾ç½®æµ‹è¯•æ•°æ®åº“"""
    engine = create_engine("sqlite:///test_real_scan.db", echo=False)
    Base.metadata.create_all(engine)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    return SessionLocal()


def create_test_cluster(db_session):
    """åˆ›å»ºæµ‹è¯•é›†ç¾¤é…ç½®"""
    cluster = Cluster(
        name="CDPæµ‹è¯•é›†ç¾¤",
        description="è¿æ¥åˆ°192.168.0.105çš„çœŸå®CDPé›†ç¾¤",
        hive_host="192.168.0.105",
        hive_port=10000,
        hive_database="default",
        hive_metastore_url="mysql://root:!qaz2wsx3edC@192.168.0.105:3306/hive",
        hdfs_namenode_url="hdfs://nameservice1",
        hdfs_user="hdfs",
        small_file_threshold=128 * 1024 * 1024,  # 128MB
        status="active",
    )

    db_session.add(cluster)
    db_session.commit()
    db_session.refresh(cluster)
    return cluster


def test_real_scanning():
    """æ‰§è¡ŒçœŸå®æ•°æ®æ‰«ææµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹çœŸå®æ•°æ®æ‰«ææµ‹è¯•")

    try:
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        db_session = setup_test_database()
        cluster = create_test_cluster(db_session)
        logger.info(f"åˆ›å»ºæµ‹è¯•é›†ç¾¤: {cluster.name} (ID: {cluster.id})")

        # åˆ›å»ºæ··åˆæ‰«æå™¨
        scanner = HybridTableScanner(cluster)

        # æµ‹è¯•è¿æ¥
        logger.info("\nğŸ“¡ æµ‹è¯•è¿æ¥çŠ¶æ€...")
        connections = scanner.test_connections()

        logger.info("MetaStoreè¿æ¥:")
        metastore = connections.get("metastore", {})
        if metastore.get("status") == "success":
            logger.info(f"  âœ… è¿æ¥æˆåŠŸ")
            logger.info(f"  ğŸ“Š æ€»æ•°æ®åº“æ•°: {metastore['total_databases']}")
            logger.info(f"  ğŸ“‹ æ€»è¡¨æ•°: {metastore['total_tables']}")
            logger.info(f"  ğŸ“ ç¤ºä¾‹æ•°æ®åº“: {metastore['sample_databases']}")
        else:
            logger.error(f"  âŒ è¿æ¥å¤±è´¥: {metastore.get('message')}")
            return False

        logger.info("\nHDFSè¿æ¥:")
        hdfs = connections.get("hdfs", {})
        if hdfs.get("status") == "success":
            mode = hdfs.get("mode", "unknown")
            logger.info(f"  âœ… è¿æ¥æˆåŠŸ (æ¨¡å¼: {mode})")
            if mode == "mock":
                logger.info(
                    f"  ğŸ”„ æ³¨æ„: ä½¿ç”¨Mockæ¨¡å¼ï¼ŒçœŸå®HDFSè¿æ¥å¤±è´¥: {hdfs.get('real_hdfs_error')}"
                )
            else:
                logger.info(f"  ğŸ¯ WebHDFS URL: {hdfs.get('webhdfs_url')}")
        else:
            logger.error(f"  âŒ è¿æ¥å¤±è´¥: {hdfs.get('message')}")
            return False

        # æ‰«ææµ‹è¯•æ•°æ®åº“
        test_database = "default"
        logger.info(f"\nğŸ” å¼€å§‹æ‰«ææ•°æ®åº“: {test_database}")

        scan_result = scanner.scan_database_tables(
            db_session, test_database, max_tables=5  # é™åˆ¶æ‰«æ5ä¸ªè¡¨è¿›è¡Œæ¼”ç¤º
        )

        logger.info("\nğŸ“Š æ‰«æç»“æœ:")
        logger.info(f"  æ•°æ®åº“: {scan_result['database_name']}")
        logger.info(f"  æ‰«ææ¨¡å¼: {scan_result['hdfs_mode']}")
        logger.info(f"  å·²æ‰«æè¡¨æ•°: {scan_result['tables_scanned']}")
        logger.info(f"  æ€»æ–‡ä»¶æ•°: {scan_result['total_files']}")
        logger.info(f"  å°æ–‡ä»¶æ•°: {scan_result['total_small_files']}")
        logger.info(f"  æ‰«æç”¨æ—¶: {scan_result['scan_duration']:.2f} ç§’")

        if scan_result["errors"]:
            logger.warning(f"  é”™è¯¯ä¿¡æ¯: {scan_result['errors']}")

        # æŸ¥è¯¢å¹¶å±•ç¤ºæ‰«æç»“æœ
        logger.info("\nğŸ“‹ æ‰«æè¯¦ç»†ç»“æœ:")
        table_metrics = (
            db_session.query(TableMetric)
            .filter(TableMetric.cluster_id == cluster.id)
            .all()
        )

        for metric in table_metrics:
            logger.info(f"\n  è¡¨: {metric.database_name}.{metric.table_name}")
            logger.info(f"    è·¯å¾„: {metric.table_path}")
            logger.info(f"    æ€»æ–‡ä»¶: {metric.total_files}")
            logger.info(f"    å°æ–‡ä»¶: {metric.small_files}")
            logger.info(
                f"    å°æ–‡ä»¶æ¯”ä¾‹: {(metric.small_files/metric.total_files*100):.1f}%"
                if metric.total_files > 0
                else "N/A"
            )
            logger.info(f"    æ€»å¤§å°: {metric.total_size / (1024**3):.2f} GB")
            logger.info(f"    å¹³å‡æ–‡ä»¶å¤§å°: {metric.avg_file_size / (1024**2):.2f} MB")
            logger.info(f"    æ˜¯å¦åˆ†åŒºè¡¨: {'æ˜¯' if metric.is_partitioned else 'å¦'}")

            if metric.is_partitioned:
                logger.info(f"    åˆ†åŒºæ•°: {metric.partition_count}")

                # æ˜¾ç¤ºåˆ†åŒºè¯¦æƒ…
                partitions = (
                    db_session.query(PartitionMetric)
                    .filter(PartitionMetric.table_metric_id == metric.id)
                    .all()
                )

                for partition in partitions[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªåˆ†åŒº
                    logger.info(f"      åˆ†åŒº: {partition.partition_name}")
                    logger.info(f"        æ–‡ä»¶æ•°: {partition.file_count}")
                    logger.info(f"        å°æ–‡ä»¶æ•°: {partition.small_file_count}")
                    logger.info(
                        f"        å¤§å°: {partition.total_size / (1024**2):.2f} MB"
                    )

                if len(partitions) > 3:
                    logger.info(f"      ... è¿˜æœ‰ {len(partitions) - 3} ä¸ªåˆ†åŒº")

        # ç”Ÿæˆå°æ–‡ä»¶åˆ†ææŠ¥å‘Š
        if table_metrics:
            logger.info("\nğŸ“ˆ å°æ–‡ä»¶åˆ†ææŠ¥å‘Š:")
            total_tables = len(table_metrics)
            total_files = sum(m.total_files for m in table_metrics)
            total_small_files = sum(m.small_files for m in table_metrics)

            if total_files > 0:
                overall_ratio = (total_small_files / total_files) * 100
                logger.info(f"  æ€»è®¡: {total_tables} ä¸ªè¡¨ï¼Œ{total_files} ä¸ªæ–‡ä»¶")
                logger.info(f"  å°æ–‡ä»¶: {total_small_files} ä¸ª ({overall_ratio:.1f}%)")

                # æ‰¾å‡ºå°æ–‡ä»¶é—®é¢˜æœ€ä¸¥é‡çš„è¡¨
                problematic_tables = [
                    m
                    for m in table_metrics
                    if m.total_files > 0 and (m.small_files / m.total_files) > 0.5
                ]

                if problematic_tables:
                    logger.info(f"\n  ğŸš¨ éœ€è¦ä¼˜åŒ–çš„è¡¨ (å°æ–‡ä»¶æ¯”ä¾‹>50%):")
                    for table in sorted(
                        problematic_tables,
                        key=lambda x: x.small_files / x.total_files,
                        reverse=True,
                    )[:5]:
                        ratio = (table.small_files / table.total_files) * 100
                        logger.info(
                            f"    {table.database_name}.{table.table_name}: {ratio:.1f}%"
                        )

        logger.info("\nğŸ‰ çœŸå®æ•°æ®æ‰«ææµ‹è¯•å®Œæˆ!")
        return True

    except Exception as e:
        logger.error(f"ğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False
    finally:
        if "db_session" in locals():
            db_session.close()


if __name__ == "__main__":
    success = test_real_scanning()
    if success:
        print("\nâœ… çœŸå®æ•°æ®æ‰«ææµ‹è¯•æˆåŠŸ! ç³»ç»Ÿå¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
        print("\nğŸ’¡ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° test_real_scan.db æ•°æ®åº“æ–‡ä»¶ä¸­ã€‚")
    else:
        print("\nâŒ çœŸå®æ•°æ®æ‰«ææµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        sys.exit(1)
