from typing import Dict, List, Optional
import time
import json
import threading
from datetime import datetime
from sqlalchemy.orm import Session
from app.models.scan_task import ScanTask
from app.models.cluster import Cluster
from app.monitor.hybrid_table_scanner import HybridTableScanner
from app.monitor.mysql_hive_connector import MySQLHiveMetastoreConnector
from app.schemas.scan_task import ScanTaskLog
from app.config.database import SessionLocal


class ScanTaskManager:
    """æ‰«æä»»åŠ¡ç®¡ç†å™¨ï¼Œæ”¯æŒè¿›åº¦è¿½è¸ªå’Œæ—¥å¿—è®°å½•"""
    
    def __init__(self):
        self.active_tasks: Dict[str, ScanTask] = {}
        self.task_logs: Dict[str, List[ScanTaskLog]] = {}
        self._lock = threading.Lock()
    
    def create_scan_task(
        self, 
        db: Session,
        cluster_id: int,
        task_type: str,
        task_name: str,
        target_database: Optional[str] = None,
        target_table: Optional[str] = None
    ) -> ScanTask:
        """åˆ›å»ºæ‰«æä»»åŠ¡"""
        task = ScanTask(
            cluster_id=cluster_id,
            task_type=task_type,
            task_name=task_name,
            status='pending'
        )
        
        db.add(task)
        db.commit()
        db.refresh(task)
        
        with self._lock:
            self.active_tasks[task.task_id] = task
            self.task_logs[task.task_id] = []
        
        return task
    
    def get_task(self, task_id: str) -> Optional[ScanTask]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        with self._lock:
            return self.active_tasks.get(task_id)
    
    def get_task_logs(self, task_id: str) -> List[ScanTaskLog]:
        """è·å–ä»»åŠ¡æ—¥å¿—"""
        with self._lock:
            return self.task_logs.get(task_id, [])
    
    def add_log(
        self,
        task_id: str,
        level: str,
        message: str,
        database_name: Optional[str] = None,
        table_name: Optional[str] = None,
        db: Optional[Session] = None,
    ):
        """æ·»åŠ ä»»åŠ¡æ—¥å¿—ï¼ˆå†…å­˜ + å¯é€‰æŒä¹…åŒ–ï¼‰"""
        log_entry = ScanTaskLog(
            timestamp=datetime.utcnow(),
            level=level,
            message=message,
            database_name=database_name,
            table_name=table_name
        )
        
        with self._lock:
            if task_id in self.task_logs:
                self.task_logs[task_id].append(log_entry)
                # åªä¿ç•™æœ€è¿‘100æ¡æ—¥å¿—
                if len(self.task_logs[task_id]) > 100:
                    self.task_logs[task_id] = self.task_logs[task_id][-100:]
        
        # å¯é€‰æŒä¹…åŒ–åˆ°æ•°æ®åº“
        if db is not None:
            try:
                from app.models.scan_task import ScanTask as ScanTaskModel
                from app.models.scan_task_log import ScanTaskLogDB
                db_task = db.query(ScanTaskModel).filter(ScanTaskModel.task_id == task_id).first()
                if db_task:
                    db_row = ScanTaskLogDB(
                        scan_task_id=db_task.id,
                        level=level,
                        message=message,
                        database_name=database_name,
                        table_name=table_name,
                    )
                    db.add(db_row)
                    db.commit()
            except Exception:
                # æŒä¹…åŒ–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                pass
    
    def update_task_progress(
        self, 
        db: Session,
        task_id: str, 
        completed_items: int = None,
        current_item: str = None,
        total_items: int = None,
        status: str = None,
        error_message: str = None,
        total_tables_scanned: int = None,
        total_files_found: int = None,
        total_small_files: int = None,
        estimated_remaining_seconds: int = None,
    ):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        with self._lock:
            task = self.active_tasks.get(task_id)
            if not task:
                return
            
            if completed_items is not None:
                task.completed_items = completed_items
            if current_item is not None:
                task.current_item = current_item
            if total_items is not None:
                task.total_items = total_items
            if status is not None:
                task.status = status
            if error_message is not None:
                task.error_message = error_message
            if total_tables_scanned is not None:
                task.total_tables_scanned = total_tables_scanned
            if total_files_found is not None:
                task.total_files_found = total_files_found
            if total_small_files is not None:
                task.total_small_files = total_small_files
            
            # æ›´æ–°æ•°æ®åº“
            db_task = db.query(ScanTask).filter(ScanTask.task_id == task_id).first()
            if db_task:
                for key, value in task.__dict__.items():
                    if not key.startswith('_') and value is not None:
                        setattr(db_task, key, value)
                db.commit()

    def safe_update_progress(self, db: Session, task_id: str, **kwargs):
        """å®‰å…¨æ›´æ–°è¿›åº¦ï¼š
        - ä»…é€ä¼ å·²æ”¯æŒå­—æ®µ
        - å¿½ç•¥æœªçŸ¥å­—æ®µå¹¶è®°å½• WARN æ—¥å¿—
        - ä»»æ„å¼‚å¸¸å°†è¢«æ•è·å¹¶è®°å½•ï¼Œä¸ä¸­æ–­ä¸»æµç¨‹
        """
        allowed = {
            'completed_items', 'current_item', 'total_items', 'status', 'error_message',
            'total_tables_scanned', 'total_files_found', 'total_small_files',
            'estimated_remaining_seconds'
        }
        passed = {k: v for k, v in kwargs.items() if k in allowed and v is not None}
        ignored = [k for k in kwargs.keys() if k not in allowed]
        if ignored:
            try:
                self.add_log(task_id, 'WARN', f'safe_update_progress å¿½ç•¥æœªçŸ¥å­—æ®µ: {ignored}', db=db)
            except Exception:
                pass
        try:
            return self.update_task_progress(db, task_id, **passed)
        except Exception as e:
            try:
                self.add_log(task_id, 'WARN', f'safe_update_progress æ›´æ–°å¤±è´¥: {e}', db=db)
            except Exception:
                pass
            return None
    
    def complete_task(self, db: Session, task_id: str, success: bool = True, error_message: str = None):
        """å®Œæˆä»»åŠ¡"""
        with self._lock:
            task = self.active_tasks.get(task_id)
            if not task:
                return
            
            task.status = 'completed' if success else 'failed'
            task.end_time = datetime.utcnow()
            task.duration = (task.end_time - task.start_time).total_seconds()
            
            if error_message:
                task.error_message = error_message
            
            # æ›´æ–°æ•°æ®åº“
            db_task = db.query(ScanTask).filter(ScanTask.task_id == task_id).first()
            if db_task:
                db_task.status = task.status
                db_task.end_time = task.end_time
                db_task.duration = task.duration
                if error_message:
                    db_task.error_message = error_message
                db.commit()
    
    from typing import Optional
    def scan_cluster_with_progress(self, db: Session, cluster_id: int, max_tables_per_db: Optional[int] = None, strict_real: bool = False) -> str:
        """æ‰§è¡Œé›†ç¾¤æ‰«æï¼ˆå¸¦è¿›åº¦è¿½è¸ªï¼‰"""
        # è·å–é›†ç¾¤ä¿¡æ¯
        cluster = db.query(Cluster).filter(Cluster.id == cluster_id).first()
        if not cluster:
            raise ValueError("Cluster not found")
        
        # åˆ›å»ºä»»åŠ¡
        task = self.create_scan_task(
            db, 
            cluster_id, 
            'cluster', 
            f'æ‰«æé›†ç¾¤: {cluster.name}'
        )
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œæ‰«æ
        def run_scan():
            # ä¸ºåå°çº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼Œé¿å…è·¨çº¿ç¨‹å¤ç”¨åŒä¸€ Session å¯¼è‡´å´©æºƒ
            db_thread = SessionLocal()
            try:
                # é‡æ–°è·å– cluster ä¸ taskï¼Œç¡®ä¿åœ¨è¯¥ä¼šè¯ä¸Šä¸‹æ–‡ä¸­æ‰˜ç®¡
                cluster_t = db_thread.query(Cluster).filter(Cluster.id == cluster.id).first() if cluster else None
                task_t = db_thread.query(ScanTask).filter(ScanTask.id == task.id).first() if task else None
                if not cluster_t or not task_t:
                    raise RuntimeError('Failed to load task/cluster in thread-local session')

                self._execute_cluster_scan(db_thread, task_t, cluster_t, max_tables_per_db, strict_real)
            except Exception as e:
                try:
                    self.add_log(task.task_id, 'ERROR', f'æ‰«æå¤±è´¥: {str(e)}', db=db_thread)
                finally:
                    self.complete_task(db_thread, task.task_id, success=False, error_message=str(e))
            finally:
                try:
                    db_thread.close()
                except Exception:
                    pass
        
        thread = threading.Thread(target=run_scan)
        thread.daemon = True
        thread.start()
        
        return task.task_id
    
    def _execute_cluster_scan(self, db: Session, task: ScanTask, cluster: Cluster, max_tables_per_db: Optional[int], strict_real: bool):
        """æ‰§è¡Œå®é™…çš„é›†ç¾¤æ‰«æ"""
        scan_start_time = time.time()
        self.add_log(task.task_id, 'INFO', f'ğŸš€ å¼€å§‹æ‰«æé›†ç¾¤: {cluster.name}', db=db)
        self.safe_update_progress(db, task.task_id, status='running')
        
        try:
            # æ­¥éª¤1: æµ‹è¯•è¿æ¥
            self.add_log(task.task_id, 'INFO', f'ğŸ”— æ­£åœ¨è¿æ¥MetaStore: {cluster.hive_metastore_url}', db=db)
            
            # è·å–æ‰€æœ‰æ•°æ®åº“
            databases = []
            metastore_connect_start = time.time()
            try:
                with MySQLHiveMetastoreConnector(cluster.hive_metastore_url) as connector:
                    databases = connector.get_databases()
                metastore_connect_time = time.time() - metastore_connect_start
                self.add_log(task.task_id, 'INFO', f'âœ… MetaStoreè¿æ¥æˆåŠŸ (è€—æ—¶: {metastore_connect_time:.2f}ç§’)', db=db)
                self.add_log(task.task_id, 'INFO', f'ğŸ“Š å‘ç° {len(databases)} ä¸ªæ•°æ®åº“: {", ".join(databases[:5])}{"..." if len(databases) > 5 else ""}', db=db)
            except Exception as conn_error:
                self.add_log(task.task_id, 'ERROR', f'âŒ MetaStoreè¿æ¥å¤±è´¥: {str(conn_error)}', db=db)
                self.add_log(task.task_id, 'INFO', f'ğŸ’¡ å»ºè®®æ£€æŸ¥: 1) ç½‘ç»œè¿é€šæ€§ 2) æ•°æ®åº“æœåŠ¡çŠ¶æ€ 3) ç”¨æˆ·æƒé™', db=db)
                raise conn_error
            
            # æ­¥éª¤2: åˆå§‹åŒ–æ‰«æå™¨å’ŒHDFSè¿æ¥
            self.add_log(task.task_id, 'INFO', f'ğŸ”— æ­£åœ¨è¿æ¥HDFS: {cluster.hdfs_namenode_url}', db=db)
            scanner = HybridTableScanner(cluster)
            hdfs_connect_start = time.time()
            
            # æµ‹è¯•HDFSè¿æ¥
            scanner._initialize_hdfs_scanner()
            hdfs_ok = False
            try:
                hdfs_ok = scanner.hdfs_scanner.connect() if scanner.hdfs_scanner else False
                hdfs_connect_time = time.time() - hdfs_connect_start
                if hdfs_ok:
                    self.add_log(task.task_id, 'INFO', f'âœ… HDFSè¿æ¥æˆåŠŸ (è€—æ—¶: {hdfs_connect_time:.2f}ç§’)', db=db)
                    scanner.hdfs_scanner.disconnect()
                else:
                    if strict_real:
                        raise Exception('HDFSè¿æ¥å¤±è´¥ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰ï¼Œä¸­æ­¢æ‰«æ')
                    self.add_log(task.task_id, 'WARN', f'âš ï¸ HDFSè¿æ¥å¤±è´¥ï¼Œå¯ç”¨Mockæ¨¡å¼ç»§ç»­æ‰«æ', db=db)
            except Exception as hdfs_error:
                if strict_real:
                    raise
                self.add_log(task.task_id, 'WARN', f'âš ï¸ HDFSè¿æ¥å¤±è´¥: {str(hdfs_error)}', db=db)
                self.add_log(task.task_id, 'INFO', f'ğŸ”„ å¯ç”¨Mock HDFSæ¨¡å¼è¿›è¡Œæ¼”ç¤ºæ‰«æ', db=db)
            
            # æ­¥éª¤3: å¼€å§‹æ‰¹é‡æ‰«æ
            self.safe_update_progress(db, task.task_id, total_items=len(databases))
            estimated_total_time = len(databases) * 3  # ä¼°ç®—æ¯ä¸ªæ•°æ®åº“3ç§’
            self.add_log(task.task_id, 'INFO', f'â±ï¸ é¢„è®¡æ€»æ‰«ææ—¶é—´: {estimated_total_time}ç§’ï¼Œæ¯æ•°æ®åº“é™åˆ¶æ‰«æ{max_tables_per_db}å¼ è¡¨', db=db)
            
            total_tables_scanned = 0
            total_files_found = 0
            total_small_files = 0
            successful_databases = 0
            failed_databases = 0
            
            # æ‰«ææ¯ä¸ªæ•°æ®åº“
            for i, database_name in enumerate(databases, 1):
                db_scan_start = time.time()
                self.safe_update_progress(
                    db, task.task_id, 
                    completed_items=i-1,
                    current_item=f'æ‰«ææ•°æ®åº“: {database_name}'
                )
                self.add_log(task.task_id, 'INFO', f'ğŸ“ [{i}/{len(databases)}] å¼€å§‹æ‰«ææ•°æ®åº“: {database_name}', database_name=database_name, db=db)
                
                try:
                    # æ‰«ææ•°æ®åº“ä¸­çš„è¡¨
                    result = scanner.scan_database_tables(db, database_name, max_tables=max_tables_per_db, strict_real=strict_real)
                    db_scan_time = time.time() - db_scan_start
                    
                    tables_scanned = result.get('tables_scanned', 0)
                    files_found = result.get('total_files', 0)
                    small_files = result.get('total_small_files', 0)
                    successful_tables = result.get('successful_tables', 0)
                    failed_tables = result.get('failed_tables', 0)
                    partitioned_tables = result.get('partitioned_tables', 0)
                    hdfs_mode = result.get('hdfs_mode', 'unknown')
                    scan_errors = result.get('errors', [])
                    metastore_time = result.get('metastore_query_time', 0)
                    hdfs_connect_time = result.get('hdfs_connect_time', 0)
                    original_table_count = result.get('original_table_count', 0)
                    filtered_table_count = result.get('filtered_table_count', 0)
                    limited_by_count = result.get('limited_by_count', 0)
                    avg_table_scan_time = result.get('avg_table_scan_time', 0)
                    
                    total_tables_scanned += tables_scanned
                    total_files_found += files_found
                    total_small_files += small_files
                    
                    # è®°å½•è¯¦ç»†çš„æ‰«æè¿‡ç¨‹ä¿¡æ¯
                    self.add_log(task.task_id, 'INFO', f'  â””â”€ MetaStoreæŸ¥è¯¢: {metastore_time:.2f}ç§’, å‘ç°{original_table_count}å¼ è¡¨', database_name=database_name, db=db)
                    
                    if limited_by_count > 0:
                        self.add_log(task.task_id, 'INFO', f'  â””â”€ è¡¨æ•°é‡é™åˆ¶: è·³è¿‡{limited_by_count}å¼ è¡¨ (é™åˆ¶{max_tables_per_db}å¼ /æ•°æ®åº“)', database_name=database_name, db=db)
                    
                    if hdfs_mode != 'real':
                        self.add_log(task.task_id, 'INFO', f'  â””â”€ HDFSè¿æ¥: {hdfs_connect_time:.2f}ç§’ ({hdfs_mode}æ¨¡å¼)', database_name=database_name, db=db)
                    
                    # è®¡ç®—å°æ–‡ä»¶æ¯”ä¾‹
                    small_file_ratio = (small_files / files_found * 100) if files_found > 0 else 0
                    
                    # ç”Ÿæˆè¯¦ç»†çš„å®Œæˆæ—¥å¿—
                    status_parts = []
                    if successful_tables > 0:
                        status_parts.append(f'{successful_tables}è¡¨æˆåŠŸ')
                    if failed_tables > 0:
                        status_parts.append(f'{failed_tables}è¡¨å¤±è´¥')
                    if partitioned_tables > 0:
                        status_parts.append(f'{partitioned_tables}ä¸ªåˆ†åŒºè¡¨')
                    
                    log_message = f'âœ… æ•°æ®åº“ {database_name} æ‰«æå®Œæˆ: {", ".join(status_parts)}, {files_found}æ–‡ä»¶, {small_files}å°æ–‡ä»¶'
                    if files_found > 0:
                        log_message += f' ({small_file_ratio:.1f}%)'
                    
                    if hdfs_mode == 'mock':
                        log_message += ' [Mockæ¨¡å¼]'
                    
                    log_message += f' (è€—æ—¶: {db_scan_time:.1f}ç§’, å¹³å‡{avg_table_scan_time:.2f}ç§’/è¡¨)'
                    
                    self.add_log(task.task_id, 'INFO', log_message, database_name=database_name, db=db)
                    
                    # è®°å½•é‡è¦çš„è­¦å‘Šå’Œé”™è¯¯ï¼ˆè¿‡æ»¤æ‰ä¸é‡è¦çš„ï¼‰
                    important_errors = [e for e in scan_errors if any(keyword in e for keyword in ['å¤±è´¥', 'é”™è¯¯', 'è¿‡é«˜', 'Error', 'Failed'])]
                    for error in important_errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé‡è¦é”™è¯¯
                        if 'å°æ–‡ä»¶æ¯”ä¾‹è¿‡é«˜' in error:
                            self.add_log(task.task_id, 'WARN', f'  âš ï¸ {error}', database_name=database_name, db=db)
                        else:
                            self.add_log(task.task_id, 'ERROR', f'  âŒ {error}', database_name=database_name, db=db)
                    
                    # æ›´æ–°è¿›åº¦å’Œå‰©ä½™æ—¶é—´ä¼°ç®—
                    remaining_dbs = len(databases) - i
                    avg_time_per_db = (time.time() - scan_start_time) / i
                    estimated_remaining = remaining_dbs * avg_time_per_db
                    
                    self.safe_update_progress(
                        db, task.task_id,
                        completed_items=i,
                        estimated_remaining_seconds=int(estimated_remaining)
                    )
                    # ç¡®è®¤è¯¥æ•°æ®åº“æ‰«ææˆåŠŸ
                    successful_databases += 1
                    
                except Exception as db_error:
                    failed_databases += 1
                    db_scan_time = time.time() - db_scan_start
                    self.add_log(
                        task.task_id,
                        'ERROR',
                        f'âŒ æ•°æ®åº“ {database_name} æ‰«æå¤±è´¥: {str(db_error)} (è€—æ—¶: {db_scan_time:.1f}ç§’)',
                        database_name=database_name,
                        db=db,
                    )
                    # æä¾›å…·ä½“çš„é”™è¯¯è¯Šæ–­å»ºè®®
                    if "permission" in str(db_error).lower():
                        self.add_log(task.task_id, 'INFO', f'  ğŸ’¡ å»ºè®®æ£€æŸ¥HDFSç”¨æˆ·æƒé™å’Œè®¿é—®ç­–ç•¥', database_name=database_name, db=db)
                    elif "connection" in str(db_error).lower():
                        self.add_log(task.task_id, 'INFO', f'  ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿é€šæ€§å’ŒæœåŠ¡çŠ¶æ€', database_name=database_name, db=db)
            
            # æœ€ç»ˆç»Ÿè®¡å’Œæ€§èƒ½æŒ‡æ ‡
            total_scan_time = time.time() - scan_start_time
            success_rate = (successful_databases / len(databases)) * 100 if databases else 0
            avg_time_per_db = total_scan_time / len(databases) if databases else 0
            avg_time_per_table = total_scan_time / total_tables_scanned if total_tables_scanned else 0
            
            self.safe_update_progress(
                db, task.task_id,
                completed_items=len(databases),
                total_tables_scanned=total_tables_scanned,
                total_files_found=total_files_found,
                total_small_files=total_small_files
            )
            
            # ç”Ÿæˆè¯¦ç»†çš„å®ŒæˆæŠ¥å‘Š
            overall_small_file_ratio = (total_small_files / total_files_found * 100) if total_files_found > 0 else 0
            completion_message = f'ğŸ‰ é›†ç¾¤æ‰«æå®Œæˆ! æ€»è®¡: {total_tables_scanned}è¡¨, {total_files_found}æ–‡ä»¶, {total_small_files}å°æ–‡ä»¶ ({overall_small_file_ratio:.1f}%)'
            self.add_log(task.task_id, 'INFO', completion_message, db=db)
            
            # æ€§èƒ½ç»Ÿè®¡
            self.add_log(task.task_id, 'INFO', f'ğŸ“ˆ æ‰«æç»Ÿè®¡: è€—æ—¶{total_scan_time:.1f}ç§’, æˆåŠŸç‡{success_rate:.1f}%, å¹³å‡æ¯è¡¨{avg_time_per_table:.2f}ç§’', db=db)
            self.add_log(task.task_id, 'INFO', f'ğŸ“Š å¤„ç†æ•ˆç‡: {successful_databases}ä¸ªæ•°æ®åº“æˆåŠŸ, {failed_databases}ä¸ªå¤±è´¥', db=db)
            
            # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
            if overall_small_file_ratio > 70:
                self.add_log(task.task_id, 'WARN', f'âš ï¸ å°æ–‡ä»¶æ¯”ä¾‹è¿‡é«˜({overall_small_file_ratio:.1f}%)ï¼Œå»ºè®®ç«‹å³æ‰§è¡Œåˆå¹¶ä¼˜åŒ–', db=db)
            elif overall_small_file_ratio > 50:
                self.add_log(task.task_id, 'INFO', f'ğŸ’¡ å»ºè®®å¯¹å°æ–‡ä»¶æ¯”ä¾‹è¾ƒé«˜çš„è¡¨è¿›è¡Œåˆå¹¶å¤„ç†', db=db)
            
            self.complete_task(db, task.task_id, success=True)
            
        except Exception as e:
            total_scan_time = time.time() - scan_start_time
            error_msg = str(e)
            self.add_log(task.task_id, 'ERROR', f'ğŸ’¥ é›†ç¾¤æ‰«æå¤±è´¥: {error_msg} (è¿è¡Œæ—¶é—´: {total_scan_time:.1f}ç§’)', db=db)
            
            # æ ¹æ®é”™è¯¯ç±»å‹æä¾›å…·ä½“å»ºè®®
            if "Failed to connect to MetaStore" in error_msg:
                self.add_log(task.task_id, 'INFO', f'ğŸ”§ MetaStoreè¿æ¥é—®é¢˜æ’æŸ¥:', db=db)
                self.add_log(task.task_id, 'INFO', f'  1. æ£€æŸ¥ç½‘ç»œè¿é€šæ€§: ping {cluster.hive_metastore_url}', db=db)
                self.add_log(task.task_id, 'INFO', f'  2. éªŒè¯æ•°æ®åº“æœåŠ¡çŠ¶æ€å’Œç«¯å£å¼€æ”¾', db=db)
                self.add_log(task.task_id, 'INFO', f'  3. ç¡®è®¤ç”¨æˆ·åå¯†ç å’Œæ•°æ®åº“æƒé™', db=db)
            elif "timeout" in error_msg.lower():
                self.add_log(task.task_id, 'INFO', f'â±ï¸ è¿æ¥è¶…æ—¶é—®é¢˜å¯èƒ½åŸå› :', db=db)
                self.add_log(task.task_id, 'INFO', f'  1. ç½‘ç»œå»¶è¿Ÿè¿‡é«˜æˆ–é˜²ç«å¢™é˜»æŒ¡', db=db)
                self.add_log(task.task_id, 'INFO', f'  2. ç›®æ ‡æœåŠ¡è¿‡è½½æˆ–å“åº”ç¼“æ…¢', db=db)
            
            self.complete_task(db, task.task_id, success=False, error_message=error_msg)


# å…¨å±€ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
scan_task_manager = ScanTaskManager()
