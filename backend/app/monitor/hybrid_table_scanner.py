import logging
from typing import Dict, List, Optional
from datetime import datetime
import time
from sqlalchemy.orm import Session

from app.monitor.mysql_hive_connector import MySQLHiveMetastoreConnector
from app.monitor.webhdfs_scanner import WebHDFSScanner
from app.monitor.enhanced_webhdfs_scanner import EnhancedWebHDFSScanner
from app.monitor.intelligent_hybrid_scanner import IntelligentHybridScanner
from app.monitor.mock_hdfs_scanner import MockHDFSScanner
from app.models.cluster import Cluster
from app.models.table_metric import TableMetric
from app.models.partition_metric import PartitionMetric

logger = logging.getLogger(__name__)

class HybridTableScanner:
    """
    æ··åˆç‰ˆæœ¬çš„è¡¨æ‰«ææœåŠ¡
    - ä½¿ç”¨çœŸå®çš„Hive MetaStoreè·å–è¡¨ä¿¡æ¯
    - æ™ºèƒ½é€‰æ‹©HDFSæ‰«æå™¨ï¼šä¼˜å…ˆä½¿ç”¨çœŸå®HDFSï¼Œå¤±è´¥æ—¶å›é€€åˆ°Mockæ¨¡å¼
    """
    
    def __init__(self, cluster: Cluster):
        self.cluster = cluster
        self.hive_connector = MySQLHiveMetastoreConnector(cluster.hive_metastore_url)
        
        # åˆå§‹åŒ–HDFSæ‰«æå™¨ï¼Œä½†åœ¨è¿è¡Œæ—¶æ™ºèƒ½é€‰æ‹©
        self.hdfs_scanner = None
        self.hdfs_mode = None  # 'real' or 'mock'
        
    def _initialize_hdfs_scanner(self):
        """æ™ºèƒ½åˆå§‹åŒ–HDFSæ‰«æå™¨"""
        if self.hdfs_scanner is not None:
            return  # å·²ç»åˆå§‹åŒ–è¿‡
        
        # ç›´æ¥å°è¯•ä½¿ç”¨WebHDFSæ‰«æå™¨ï¼ˆä¸ä½¿ç”¨Kerberosï¼‰
        try:
            # æ£€æµ‹ç«¯å£ä»¥ç¡®å®šæœåŠ¡ç±»å‹
            if ':14000' in self.cluster.hdfs_namenode_url:
                port = 14000
                service_name = "HttpFS"
            else:
                port = 9870
                service_name = "WebHDFS"
            
            logger.info(f"å°è¯•è¿æ¥{service_name}æœåŠ¡: {self.cluster.hdfs_namenode_url}:{port}")
            
            # ä½¿ç”¨WebHDFSæ‰«æå™¨ï¼ˆä¸ä½¿ç”¨Kerberosè®¤è¯ï¼‰
            webhdfs_scanner = WebHDFSScanner(
                namenode_url=self.cluster.hdfs_namenode_url,
                user=self.cluster.hdfs_user,
                webhdfs_port=port,
                password=self.cluster.hdfs_password
            )
            
            # æµ‹è¯•è¿æ¥
            test_result = webhdfs_scanner.test_connection()
            
            if test_result['status'] == 'success':
                logger.info(f"âœ… WebHDFSæ‰«æå™¨è¿æ¥æˆåŠŸ")
                logger.info(f"   æœåŠ¡ç±»å‹: {test_result.get('service_type')}")
                logger.info(f"   è®¤è¯æ–¹å¼: {test_result.get('auth_method')}")
                logger.info(f"   æ ·ä¾‹è·¯å¾„: {test_result.get('sample_paths')}")
                
                self.hdfs_scanner = webhdfs_scanner
                self.hdfs_mode = 'webhdfs_real'
                return
            else:
                logger.warning(f"WebHDFSè¿æ¥å¤±è´¥: {test_result.get('message')}")
                
        except Exception as e:
            logger.warning(f"WebHDFSæ‰«æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # å›é€€åˆ°Mockæ¨¡å¼
        logger.info("ğŸ”„ å›é€€åˆ°Mock HDFSæ‰«æå™¨æ¨¡å¼")
        self.hdfs_scanner = MockHDFSScanner(self.cluster.hdfs_namenode_url, self.cluster.hdfs_user)
        self.hdfs_mode = 'mock'
    
    def scan_table(self, db_session: Session, database_name: str, table_info: Dict) -> Dict:
        """
        æ‰«æå•ä¸ªè¡¨çš„æ–‡ä»¶åˆ†å¸ƒ
        """
        table_name = table_info['table_name']
        table_path = table_info['table_path']
        
        logger.info(f"Scanning table {database_name}.{table_name} at {table_path}")
        
        # ç¡®ä¿HDFSæ‰«æå™¨å·²åˆå§‹åŒ–
        self._initialize_hdfs_scanner()
        
        try:
            # è·å–åˆ†åŒºä¿¡æ¯ï¼ˆå¦‚æœæ˜¯åˆ†åŒºè¡¨ï¼‰
            partitions = []
            if table_info['is_partitioned']:
                partitions = self.hive_connector.get_table_partitions(database_name, table_name)
                logger.info(f"Found {len(partitions)} partitions for table {table_name}")
            
            # æ‰«æ HDFS æ–‡ä»¶
            table_stats, partition_stats = self.hdfs_scanner.scan_table_partitions(
                table_path, partitions, self.cluster.small_file_threshold
            )
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥table_statsçš„å†…å®¹
            logger.info(f"æ‰«æç»“æœç±»å‹æ£€æŸ¥:")
            logger.info(f"  table_statsç±»å‹: {type(table_stats)}")
            logger.info(f"  table_statsé”®: {list(table_stats.keys()) if isinstance(table_stats, dict) else 'Not a dict'}")
            if isinstance(table_stats, dict):
                for key, value in table_stats.items():
                    logger.info(f"    {key}: {value} (ç±»å‹: {type(value)})")
            
            # æ ‡è®°æ•°æ®æ¥æº
            table_stats['hdfs_mode'] = self.hdfs_mode
            
            # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ç½®é»˜è®¤å€¼
            required_fields = {
                'total_files': 0,
                'small_files': 0, 
                'total_size': 0,
                'avg_file_size': 0.0,
                'scan_duration': 0.0
            }
            
            for field, default_value in required_fields.items():
                if field not in table_stats:
                    logger.warning(f"å­—æ®µ '{field}' ä¸å­˜åœ¨ï¼Œè®¾ç½®é»˜è®¤å€¼ {default_value}")
                    table_stats[field] = default_value
            
            # ä¿å­˜è¡¨çº§ç»Ÿè®¡åˆ°æ•°æ®åº“ï¼ˆåŒ…å«å¢å¼ºçš„å…ƒæ•°æ®ï¼‰
            table_metric = TableMetric(
                cluster_id=self.cluster.id,
                database_name=database_name,
                table_name=table_name,
                table_path=table_path,
                # å¢å¼ºçš„å…ƒæ•°æ®å­—æ®µ
                table_type=table_info.get('table_type'),
                storage_format=table_info.get('storage_format'),
                input_format=table_info.get('input_format'),
                output_format=table_info.get('output_format'),
                serde_lib=table_info.get('serde_lib'),
                table_owner=table_info.get('table_owner'),
                table_create_time=table_info.get('table_create_time'),
                # æ–‡ä»¶ç»Ÿè®¡
                total_files=table_stats['total_files'],
                small_files=table_stats['small_files'],
                total_size=table_stats['total_size'],
                avg_file_size=table_stats['avg_file_size'],
                # åˆ†åŒºä¿¡æ¯
                is_partitioned=1 if table_info['is_partitioned'] else 0,
                partition_count=table_info['partition_count'],
                partition_columns=None,  # TODO: è·å–åˆ†åŒºåˆ—ä¿¡æ¯
                # æ‰«æå…ƒæ•°æ®
                scan_duration=table_stats['scan_duration']
            )
            
            db_session.add(table_metric)
            db_session.flush()  # è·å– table_metric.id
            
            # ä¿å­˜åˆ†åŒºçº§ç»Ÿè®¡
            logger.info(f"åˆ†åŒºç»Ÿè®¡æ•°æ®æ£€æŸ¥: å…± {len(partition_stats)} ä¸ªåˆ†åŒº")
            for i, partition_stat in enumerate(partition_stats):
                logger.info(f"  åˆ†åŒº {i+1}: {list(partition_stat.keys()) if isinstance(partition_stat, dict) else 'Not a dict'}")
                
                # æ£€æŸ¥åˆ†åŒºç»Ÿè®¡çš„å¿…è¦å­—æ®µ
                partition_required_fields = {
                    'partition_name': 'main',
                    'partition_path': table_path,
                    'file_count': 0,
                    'small_file_count': 0,
                    'total_size': 0,
                    'avg_file_size': 0.0
                }
                
                for field, default_value in partition_required_fields.items():
                    if field not in partition_stat:
                        logger.warning(f"åˆ†åŒºå­—æ®µ '{field}' ä¸å­˜åœ¨ï¼Œè®¾ç½®é»˜è®¤å€¼ {default_value}")
                        partition_stat[field] = default_value
                
                partition_metric = PartitionMetric(
                    table_metric_id=table_metric.id,
                    partition_name=partition_stat['partition_name'],
                    partition_path=partition_stat['partition_path'],
                    file_count=partition_stat['file_count'],
                    small_file_count=partition_stat['small_file_count'],
                    total_size=partition_stat['total_size'],
                    avg_file_size=partition_stat['avg_file_size']
                )
                db_session.add(partition_metric)
            
            db_session.commit()
            
            logger.info(f"Saved metrics for table {database_name}.{table_name} "
                       f"({self.hdfs_mode} mode): {table_stats['total_files']} files, "
                       f"{table_stats['small_files']} small files")
            
            return table_stats
            
        except Exception as e:
            db_session.rollback()
            logger.error(f"Failed to scan table {database_name}.{table_name}: {e}")
            raise
    
    def scan_database_tables(self, db_session: Session, database_name: str,
                           table_filter: Optional[str] = None, max_tables: int = 10) -> Dict:
        """
        æ‰«ææŒ‡å®šæ•°æ®åº“çš„è¡¨ï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡é•¿æ—¶é—´ï¼‰
        """
        start_time = time.time()
        result = {
            'database_name': database_name,
            'tables_scanned': 0,
            'total_files': 0,
            'total_small_files': 0,
            'scan_duration': 0.0,
            'hdfs_mode': None,
            'errors': []
        }
        
        try:
            # è¿æ¥æœåŠ¡
            if not self.hive_connector.connect():
                raise Exception("Failed to connect to Hive MetaStore")
            
            # åˆå§‹åŒ–HDFSæ‰«æå™¨
            self._initialize_hdfs_scanner()
            
            if not self.hdfs_scanner.connect():
                raise Exception("Failed to connect to HDFS")
            
            result['hdfs_mode'] = self.hdfs_mode
            
            # è·å–è¡¨åˆ—è¡¨
            tables = self.hive_connector.get_tables(database_name)
            if table_filter:
                tables = [t for t in tables if table_filter in t['table_name']]
            
            # é™åˆ¶æ‰«æè¡¨çš„æ•°é‡
            tables = tables[:max_tables]
            
            logger.info(f"Found {len(tables)} tables in database {database_name} "
                       f"(limited to {max_tables} for demo)")
            
            # æ‰«ææ¯ä¸ªè¡¨
            for i, table_info in enumerate(tables, 1):
                try:
                    logger.info(f"[{i}/{len(tables)}] Scanning {table_info['table_name']}...")
                    table_result = self.scan_table(db_session, database_name, table_info)
                    result['tables_scanned'] += 1
                    result['total_files'] += table_result.get('total_files', 0)
                    result['total_small_files'] += table_result.get('small_files', 0)
                    
                except Exception as e:
                    error_msg = f"Failed to scan table {table_info['table_name']}: {e}"
                    logger.error(error_msg)
                    result['errors'].append(error_msg)
            
            result['scan_duration'] = time.time() - start_time
            
        except Exception as e:
            error_msg = f"Database {database_name} scan failed: {e}"
            logger.error(error_msg)
            result['errors'].append(error_msg)
        
        finally:
            self.hive_connector.disconnect()
            if self.hdfs_scanner:
                self.hdfs_scanner.disconnect()
        
        return result
    
    def test_connections(self) -> Dict:
        """è¯¦ç»†çš„è¿æ¥æµ‹è¯•ï¼ŒåŒ…å«è¯Šæ–­ä¿¡æ¯å’Œå»ºè®®"""
        results = {
            'overall_status': 'unknown',
            'test_time': datetime.now().isoformat(),
            'tests': {},
            'logs': [],
            'suggestions': []
        }
        
        def add_log(level: str, message: str):
            results['logs'].append({
                'level': level,
                'message': message,
                'timestamp': datetime.now().isoformat()
            })
        
        add_log('info', f'å¼€å§‹æµ‹è¯•é›†ç¾¤è¿æ¥: {self.cluster.name}')
        
        # 1. æµ‹è¯• MetaStore è¿æ¥ - ä½¿ç”¨context managerç¡®ä¿è¿æ¥ç”Ÿå‘½å‘¨æœŸç®¡ç†
        add_log('info', 'æ­£åœ¨æµ‹è¯• Hive MetaStore è¿æ¥...')
        try:
            # ä½¿ç”¨context manageræ¥ç®¡ç†è¿æ¥ç”Ÿå‘½å‘¨æœŸ
            try:
                with self.hive_connector:
                    add_log('success', f"âœ… MetaStoreè¿æ¥æˆåŠŸ: {self.cluster.hive_metastore_url}")
                    
                    # åœ¨è¿æ¥ä¸Šä¸‹æ–‡ä¸­è·å–æ•°æ®åº“åˆ—è¡¨å’ŒMetaStoreä¿¡æ¯
                    try:
                        databases = self.hive_connector.get_databases()
                        add_log('success', f"âœ… æˆåŠŸè·å–åˆ° {len(databases)} ä¸ªæ•°æ®åº“: {databases[:3]}...")
                        
                        # è·å–æ›´è¯¦ç»†çš„MetaStoreç»Ÿè®¡ä¿¡æ¯
                        with self.hive_connector._connection.cursor() as cursor:
                            cursor.execute("SELECT COUNT(*) as table_count FROM TBLS")
                            table_count = cursor.fetchone()['table_count']
                            
                            cursor.execute("SELECT COUNT(*) as db_count FROM DBS")
                            db_count = cursor.fetchone()['db_count']
                        
                        results['tests']['metastore'] = {
                            'status': 'success',
                            'message': f'è¿æ¥æˆåŠŸï¼Œæ‰¾åˆ° {len(databases)} ä¸ªæ•°æ®åº“',
                            'databases_count': len(databases),
                            'total_databases': db_count,
                            'total_tables': table_count,
                            'sample_databases': databases[:5],
                            'mode': 'mysql'
                        }
                        
                    except Exception as db_e:
                        add_log('warning', f"âš ï¸ MetaStoreåŸºç¡€è¿æ¥æˆåŠŸä½†æ•°æ®è®¿é—®å¤±è´¥: {str(db_e)}")
                        results['tests']['metastore'] = {
                            'status': 'partial', 
                            'message': f'è¿æ¥æˆåŠŸä½†æ•°æ®è®¿é—®å¤±è´¥: {str(db_e)}',
                            'mode': 'mysql'
                        }
                        results['suggestions'].append("æ£€æŸ¥æ•°æ®åº“ç”¨æˆ·æƒé™ï¼Œç¡®ä¿æœ‰è®¿é—®Hive MetaStoreè¡¨çš„SELECTæƒé™")
                        results['suggestions'].append("éªŒè¯Hive MetaStoreè¡¨ç»“æ„å®Œæ•´æ€§ï¼ˆDBS, TBLSè¡¨ç­‰ï¼‰")
                        
            except ConnectionError as conn_e:
                add_log('error', f"âŒ MetaStoreè¿æ¥å¤±è´¥: {str(conn_e)}")
                results['tests']['metastore'] = {'status': 'error', 'message': str(conn_e), 'mode': 'mysql'}
                self._add_metastore_suggestions(results, {'message': str(conn_e)})
                
        except Exception as e:
            error_msg = str(e)
            add_log('error', f"âŒ MetaStoreè¿æ¥å¼‚å¸¸: {error_msg}")
            results['tests']['metastore'] = {'status': 'error', 'message': error_msg, 'mode': 'mysql'}
            self._add_metastore_suggestions(results, {'message': error_msg})
        
        # 2. æµ‹è¯• HDFS è¿æ¥
        add_log('info', 'æ­£åœ¨æµ‹è¯• HDFS è¿æ¥...')
        try:
            # å°è¯•çœŸå®WebHDFSè¿æ¥
            add_log('info', f'å°è¯•è¿æ¥ WebHDFS: {self.cluster.hdfs_namenode_url}')
            webhdfs_scanner = WebHDFSScanner(self.cluster.hdfs_namenode_url, self.cluster.hdfs_user)
            hdfs_result = webhdfs_scanner.test_connection()
            
            if hdfs_result.get('status') == 'success':
                add_log('success', f"âœ… WebHDFSè¿æ¥æˆåŠŸ: {hdfs_result.get('webhdfs_url')}")
                hdfs_result['mode'] = 'real'
                results['tests']['hdfs'] = hdfs_result
                
                # æµ‹è¯•åŸºæœ¬çš„HDFSæ“ä½œ
                try:
                    # æµ‹è¯•æ ¹ç›®å½•è®¿é—®
                    test_result = webhdfs_scanner.test_directory_access('/')
                    if test_result:
                        add_log('success', 'âœ… HDFSæ ¹ç›®å½•è®¿é—®æƒé™æ­£å¸¸')
                    else:
                        add_log('warning', 'âš ï¸ HDFSæ ¹ç›®å½•è®¿é—®å—é™')
                        results['suggestions'].append('è¯·æ£€æŸ¥HDFSç”¨æˆ·æƒé™é…ç½®')
                except:
                    add_log('warning', 'âš ï¸ æ— æ³•æµ‹è¯•HDFSç›®å½•è®¿é—®æƒé™')
                
            else:
                add_log('warning', f"âš ï¸ WebHDFSè¿æ¥å¤±è´¥: {hdfs_result.get('message')}")
                
                # å›é€€åˆ°Mockæ¨¡å¼
                add_log('info', 'ğŸ”„ å¯ç”¨Mock HDFSæ¨¡å¼è¿›è¡Œæµ‹è¯•')
                mock_scanner = MockHDFSScanner(self.cluster.hdfs_namenode_url, self.cluster.hdfs_user)
                mock_result = mock_scanner.test_connection()
                mock_result['mode'] = 'mock'
                mock_result['real_hdfs_error'] = hdfs_result.get('message')
                results['tests']['hdfs'] = mock_result
                
                add_log('info', 'âœ… Mock HDFSæ¨¡å¼å¯ç”¨ï¼ˆå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤ºï¼‰')
                self._add_hdfs_suggestions(results, hdfs_result)
                
        except Exception as e:
            error_msg = str(e)
            add_log('error', f"âŒ HDFSè¿æ¥å¼‚å¸¸: {error_msg}")
            results['tests']['hdfs'] = {'status': 'error', 'message': error_msg}
            self._add_hdfs_suggestions(results, {'message': error_msg})
        
        # 3. ç»¼åˆè¯„ä¼°
        metastore_ok = results['tests'].get('metastore', {}).get('status') == 'success'
        hdfs_ok = results['tests'].get('hdfs', {}).get('status') == 'success'
        
        if metastore_ok and hdfs_ok:
            results['overall_status'] = 'success'
            mode = results['tests']['hdfs'].get('mode', 'unknown')
            if mode == 'real':
                add_log('success', 'ğŸ‰ æ‰€æœ‰è¿æ¥æµ‹è¯•é€šè¿‡ï¼é›†ç¾¤å¯ä»¥æ­£å¸¸ä½¿ç”¨çœŸå®æ•°æ®æ‰«æ')
            else:
                add_log('success', 'ğŸ‰ MetaStoreè¿æ¥æ­£å¸¸ï¼å°†ä½¿ç”¨Mockæ¨¡å¼è¿›è¡ŒHDFSæ•°æ®æ¼”ç¤º')
                results['suggestions'].append('å»ºè®®ä¿®å¤HDFSè¿æ¥ä»¥è·å¾—çœŸå®çš„æ–‡ä»¶ç»Ÿè®¡æ•°æ®')
        elif metastore_ok:
            results['overall_status'] = 'partial'
            add_log('warning', 'âš ï¸ MetaStoreè¿æ¥æ­£å¸¸ï¼Œä½†HDFSè¿æ¥æœ‰é—®é¢˜ã€‚å¯ä»¥æŸ¥çœ‹è¡¨ç»“æ„ä½†æ— æ³•è·å–æ–‡ä»¶ç»Ÿè®¡')
        else:
            results['overall_status'] = 'failed'
            add_log('error', 'âŒ å…³é”®è¿æ¥å¤±è´¥ï¼Œé›†ç¾¤æ— æ³•æ­£å¸¸å·¥ä½œ')
            results['suggestions'].append('è¯·å…ˆè§£å†³MetaStoreè¿æ¥é—®é¢˜ï¼Œè¿™æ˜¯é›†ç¾¤æ­£å¸¸å·¥ä½œçš„å‰æ')
        
        add_log('info', f'è¿æ¥æµ‹è¯•å®Œæˆï¼Œæ€»ä½“çŠ¶æ€: {results["overall_status"]}')
        return results
    
    def _add_metastore_suggestions(self, results: Dict, error_result: Dict):
        """æ·»åŠ MetaStoreè¿æ¥é—®é¢˜çš„è¯Šæ–­å»ºè®®"""
        error_msg = error_result.get('message', '').lower()
        
        if 'connection refused' in error_msg or 'could not connect' in error_msg:
            results['suggestions'].extend([
                'ç½‘ç»œè¿æ¥è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥ï¼š',
                '1. MySQL/PostgreSQLæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ',
                '2. é˜²ç«å¢™æ˜¯å¦å…è®¸è¿æ¥åˆ°æ•°æ®åº“ç«¯å£',
                '3. æ•°æ®åº“æœåŠ¡å™¨åœ°å€å’Œç«¯å£æ˜¯å¦æ­£ç¡®'
            ])
        elif 'access denied' in error_msg or 'authentication failed' in error_msg:
            results['suggestions'].extend([
                'æ•°æ®åº“è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š',
                '1. ç”¨æˆ·åå’Œå¯†ç æ˜¯å¦æ­£ç¡®',
                '2. æ•°æ®åº“ç”¨æˆ·æ˜¯å¦å­˜åœ¨ä¸”æœ‰æƒé™è®¿é—®Hive MetaStoreæ•°æ®åº“',
                '3. æ•°æ®åº“è¿æ¥URLæ ¼å¼æ˜¯å¦æ­£ç¡®'
            ])
        elif 'unknown database' in error_msg or 'database does not exist' in error_msg:
            results['suggestions'].extend([
                'MetaStoreæ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ï¼š',
                '1. Hive MetaStoreæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–',
                '2. æ•°æ®åº“åç§°æ˜¯å¦æ­£ç¡®ï¼ˆé€šå¸¸ä¸º"hive"æˆ–"metastore"ï¼‰',
                '3. æ˜¯å¦éœ€è¦å…ˆè¿è¡Œschematoolåˆå§‹åŒ–MetaStore'
            ])
        elif 'timeout' in error_msg:
            results['suggestions'].extend([
                'è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ï¼š',
                '1. ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®š',
                '2. æ•°æ®åº“æœåŠ¡å™¨æ˜¯å¦å“åº”ç¼“æ…¢',
                '3. æ˜¯å¦éœ€è¦å¢åŠ è¿æ¥è¶…æ—¶è®¾ç½®'
            ])
        else:
            results['suggestions'].extend([
                'MetaStoreè¿æ¥å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥ï¼š',
                '1. è¿æ¥URLæ ¼å¼ï¼šmysql://user:password@host:port/database',
                '2. æ•°æ®åº“æœåŠ¡çŠ¶æ€å’Œç½‘ç»œè¿é€šæ€§',
                '3. ç”¨æˆ·æƒé™å’Œè®¤è¯ä¿¡æ¯'
            ])
    
    def _add_hdfs_suggestions(self, results: Dict, error_result: Dict):
        """æ·»åŠ HDFSè¿æ¥é—®é¢˜çš„è¯Šæ–­å»ºè®®"""
        error_msg = error_result.get('message', '').lower()
        
        if 'connection refused' in error_msg:
            results['suggestions'].extend([
                'HDFSè¿æ¥è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥ï¼š',
                '1. NameNodeæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ',
                '2. WebHDFSæ˜¯å¦å·²å¯ç”¨ï¼ˆdfs.webhdfs.enabled=trueï¼‰',
                '3. é˜²ç«å¢™æ˜¯å¦å…è®¸è®¿é—®NameNodeç«¯å£ï¼ˆé€šå¸¸æ˜¯9870æˆ–50070ï¼‰'
            ])
        elif 'unknown host' in error_msg or 'name resolution' in error_msg:
            results['suggestions'].extend([
                'HDFSä¸»æœºåè§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š',
                '1. NameNodeä¸»æœºåæˆ–IPåœ°å€æ˜¯å¦æ­£ç¡®',
                '2. DNSè§£ææ˜¯å¦æ­£å¸¸',
                '3. /etc/hostsæ–‡ä»¶æ˜¯å¦åŒ…å«æ­£ç¡®çš„ä¸»æœºæ˜ å°„'
            ])
        elif 'unauthorized' in error_msg or 'permission denied' in error_msg:
            results['suggestions'].extend([
                'HDFSæƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥ï¼š',
                '1. HDFSç”¨æˆ·æ˜¯å¦å­˜åœ¨ä¸”æœ‰æƒé™',
                '2. Kerberosè®¤è¯æ˜¯å¦æ­£ç¡®é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰',
                '3. æ˜¯å¦éœ€è¦ä»£ç†ç”¨æˆ·é…ç½®'
            ])
        elif 'timeout' in error_msg:
            results['suggestions'].extend([
                'HDFSè¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ï¼š',
                '1. ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®š',
                '2. NameNodeæ˜¯å¦å¤„äºå®‰å…¨æ¨¡å¼',
                '3. é›†ç¾¤è´Ÿè½½æ˜¯å¦è¿‡é«˜'
            ])
        else:
            results['suggestions'].extend([
                'HDFSè¿æ¥å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥ï¼š',
                '1. NameNodeåœ°å€æ ¼å¼ï¼šhdfs://namenode:port æˆ– http://namenode:port',
                '2. WebHDFSæœåŠ¡çŠ¶æ€å’Œé…ç½®',
                '3. ç½‘ç»œè¿é€šæ€§å’Œé˜²ç«å¢™è®¾ç½®'
            ])
            
        # æ€»æ˜¯æç¤ºMockæ¨¡å¼å¯ç”¨
        results['suggestions'].append('æ³¨æ„ï¼šå³ä½¿HDFSè¿æ¥å¤±è´¥ï¼Œç³»ç»Ÿä»å¯ä½¿ç”¨Mockæ¨¡å¼è¿›è¡ŒåŠŸèƒ½æ¼”ç¤º')