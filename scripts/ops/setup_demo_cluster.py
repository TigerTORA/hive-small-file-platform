#!/usr/bin/env python3
"""
è®¾ç½®æ¼”ç¤ºé›†ç¾¤é…ç½®
åœ¨ä¸»æ•°æ®åº“ä¸­æ·»åŠ çœŸå®çš„é›†ç¾¤é…ç½®
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.models.cluster import Cluster
from app.config.database import Base
from app.config.settings import settings
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def setup_demo_cluster():
    """è®¾ç½®æ¼”ç¤ºé›†ç¾¤"""
    # ä½¿ç”¨ä¸»æ•°æ®åº“
    engine = create_engine(settings.DATABASE_URL, echo=False)
    Base.metadata.create_all(engine)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    db_session = SessionLocal()
    
    try:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒåç§°çš„é›†ç¾¤
        existing_cluster = db_session.query(Cluster).filter(
            Cluster.name == "CDPçœŸå®é›†ç¾¤"
        ).first()
        
        if existing_cluster:
            logger.info(f"é›†ç¾¤å·²å­˜åœ¨: {existing_cluster.name} (ID: {existing_cluster.id})")
            logger.info("æ›´æ–°é›†ç¾¤é…ç½®...")
            
            # æ›´æ–°é…ç½®
            existing_cluster.hive_metastore_url = "mysql://root:!qaz2wsx3edC@192.168.0.105:3306/hive"
            existing_cluster.hdfs_namenode_url = "hdfs://nameservice1"
            existing_cluster.hdfs_user = "hdfs"
            existing_cluster.small_file_threshold = 128*1024*1024
            existing_cluster.description = "è¿æ¥åˆ°192.168.0.105çš„çœŸå®CDPé›†ç¾¤ï¼Œæ”¯æŒæ··åˆHDFSæ‰«æ"
            
            db_session.commit()
            logger.info(f"âœ… é›†ç¾¤é…ç½®å·²æ›´æ–°: {existing_cluster.name}")
            return existing_cluster
        
        else:
            # åˆ›å»ºæ–°é›†ç¾¤
            cluster = Cluster(
                name="CDPçœŸå®é›†ç¾¤",
                description="è¿æ¥åˆ°192.168.0.105çš„çœŸå®CDPé›†ç¾¤ï¼Œæ”¯æŒæ··åˆHDFSæ‰«æ",
                hive_host="192.168.0.105",
                hive_port=10000,
                hive_database="default",
                hive_metastore_url="mysql://root:!qaz2wsx3edC@192.168.0.105:3306/hive",
                hdfs_namenode_url="hdfs://nameservice1",
                hdfs_user="hdfs",
                small_file_threshold=128*1024*1024,  # 128MB
                status="active",
                scan_enabled=True
            )
            
            db_session.add(cluster)
            db_session.commit()
            db_session.refresh(cluster)
            logger.info(f"âœ… åˆ›å»ºæ–°é›†ç¾¤: {cluster.name} (ID: {cluster.id})")
            return cluster
        
    except Exception as e:
        logger.error(f"è®¾ç½®æ¼”ç¤ºé›†ç¾¤å¤±è´¥: {e}")
        db_session.rollback()
        raise
    finally:
        db_session.close()

def test_demo_apis():
    """æµ‹è¯•æ¼”ç¤ºAPI"""
    import requests
    import json
    
    base_url = "http://localhost:8000"
    
    try:
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        response = requests.get(f"{base_url}/health")
        if response.status_code == 200:
            logger.info("âœ… APIæœåŠ¡æ­£å¸¸è¿è¡Œ")
        else:
            logger.warning("âš ï¸ APIæœåŠ¡å¯èƒ½æœªå¯åŠ¨")
            return
        
        # æµ‹è¯•é›†ç¾¤åˆ—è¡¨
        response = requests.get(f"{base_url}/api/v1/clusters/")
        if response.status_code == 200:
            clusters = response.json()
            logger.info(f"ğŸ“‹ å‘ç° {len(clusters)} ä¸ªé›†ç¾¤:")
            for cluster in clusters:
                logger.info(f"  - {cluster['name']} (ID: {cluster['id']})")
            
            if clusters:
                cluster_id = clusters[0]['id']
                
                # æµ‹è¯•çœŸå®è¿æ¥
                logger.info(f"\nğŸ”Œ æµ‹è¯•é›†ç¾¤ {cluster_id} çš„çœŸå®è¿æ¥...")
                response = requests.get(f"{base_url}/api/v1/tables/test-connection-real/{cluster_id}")
                if response.status_code == 200:
                    result = response.json()
                    logger.info("è¿æ¥æµ‹è¯•ç»“æœ:")
                    logger.info(f"  MetaStore: {result['connections']['metastore']['status']}")
                    logger.info(f"  HDFS: {result['connections']['hdfs']['status']} ({result['connections']['hdfs'].get('mode', 'unknown')})")
                
                # æµ‹è¯•æ•°æ®åº“åˆ—è¡¨
                logger.info(f"\nğŸ“š è·å–é›†ç¾¤ {cluster_id} çš„æ•°æ®åº“åˆ—è¡¨...")
                response = requests.get(f"{base_url}/api/v1/tables/databases/{cluster_id}")
                if response.status_code == 200:
                    result = response.json()
                    databases = result.get('databases', [])
                    logger.info(f"å‘ç° {len(databases)} ä¸ªæ•°æ®åº“: {databases[:5]}")
                    
                    if 'default' in databases:
                        # æµ‹è¯•è¡¨åˆ—è¡¨
                        logger.info(f"\nğŸ“Š è·å–defaultæ•°æ®åº“çš„è¡¨åˆ—è¡¨...")
                        response = requests.get(f"{base_url}/api/v1/tables/tables/{cluster_id}/default")
                        if response.status_code == 200:
                            result = response.json()
                            tables = result.get('tables', [])
                            logger.info(f"defaultæ•°æ®åº“æœ‰ {len(tables)} ä¸ªè¡¨")
                            for table in tables[:3]:
                                logger.info(f"  - {table['table_name']} ({'åˆ†åŒºè¡¨' if table['is_partitioned'] else 'éåˆ†åŒºè¡¨'})")
                
        else:
            logger.warning("æ— æ³•è·å–é›†ç¾¤åˆ—è¡¨")
            
    except requests.exceptions.ConnectionError:
        logger.warning("âš ï¸ æ— æ³•è¿æ¥åˆ°APIæœåŠ¡ï¼Œè¯·ç¡®ä¿åç«¯æœåŠ¡æ­£åœ¨è¿è¡Œ")
        logger.info("å¯åŠ¨å‘½ä»¤: cd backend && uvicorn app.main:app --reload --port 8000")
    except Exception as e:
        logger.error(f"APIæµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    logger.info("ğŸš€ è®¾ç½®æ¼”ç¤ºé›†ç¾¤é…ç½®")
    
    try:
        cluster = setup_demo_cluster()
        logger.info(f"\nâœ… æ¼”ç¤ºé›†ç¾¤è®¾ç½®å®Œæˆ!")
        logger.info(f"é›†ç¾¤åç§°: {cluster.name}")
        logger.info(f"é›†ç¾¤ID: {cluster.id}")
        logger.info(f"MetaStore: {cluster.hive_metastore_url}")
        logger.info(f"HDFS: {cluster.hdfs_namenode_url}")
        
        logger.info(f"\nğŸ§ª ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹APIè¿›è¡Œæµ‹è¯•:")
        logger.info(f"1. è¿æ¥æµ‹è¯•: GET /api/v1/tables/test-connection-real/{cluster.id}")
        logger.info(f"2. æ•°æ®åº“åˆ—è¡¨: GET /api/v1/tables/databases/{cluster.id}")
        logger.info(f"3. è¡¨åˆ—è¡¨: GET /api/v1/tables/tables/{cluster.id}/default")
        logger.info(f"4. çœŸå®æ‰«æ: POST /api/v1/tables/scan-real/{cluster.id}/default")
        
        logger.info(f"\nğŸŒ æµ‹è¯•APIæœåŠ¡è¿æ¥...")
        test_demo_apis()
        
    except Exception as e:
        logger.error(f"ğŸ’¥ è®¾ç½®å¤±è´¥: {e}")
        sys.exit(1)