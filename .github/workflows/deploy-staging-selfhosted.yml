name: Deploy Staging (Self-hosted)

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Image version tag (e.g., 1.0.2)'
        required: true
        default: '1.0.2'

permissions:
  contents: read

jobs:
  deploy:
    runs-on: [self-hosted, staging]
    env:
      VERSION: ${{ inputs.version }}
      IMAGE_OWNER: ${{ vars.IMAGE_OWNER || github.repository_owner }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Ensure docker compose plugin (v2)
        shell: bash
        run: |
          set -e
          mkdir -p ~/.docker/cli-plugins
          if ! docker compose version >/dev/null 2>&1; then
            curl -L -o ~/.docker/cli-plugins/docker-compose \
              https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64
            chmod +x ~/.docker/cli-plugins/docker-compose
          fi
          docker compose version

      - name: Write compose and env files (if missing)
        shell: bash
        run: |
          set -e
          cd /opt && mkdir -p hive-platform && cd hive-platform
          if [ ! -f docker-compose.prod.yml ]; then
            cat > docker-compose.prod.yml <<'EOF'
          version: '3.8'
          services:
            postgres:
              image: postgres:14
              container_name: hive-platform-db
              environment:
                POSTGRES_DB: hive_small_file_db
                POSTGRES_USER: postgres
                POSTGRES_PASSWORD: postgres
              volumes:
                - postgres_data:/var/lib/postgresql/data
              ports: [ "5432:5432" ]
              networks: [ hive-platform ]
            redis:
              image: redis:6-alpine
              container_name: hive-platform-redis
              ports: [ "6379:6379" ]
              volumes:
                - redis_data:/data
              networks: [ hive-platform ]
            api:
              image: ghcr.io/${{ env.IMAGE_OWNER }}/hive-small-file-api:${{ env.VERSION }}
              container_name: hive-platform-api
              env_file: [ backend/.env ]
              environment:
                - SENTRY_ENVIRONMENT=staging
                - AUTO_CREATE_SCHEMA=false
                - RELOAD=false
              depends_on: [ postgres, redis ]
              ports: [ "8000:8000" ]
              networks: [ hive-platform ]
            celery-worker:
              image: ghcr.io/${{ env.IMAGE_OWNER }}/hive-small-file-api:${{ env.VERSION }}
              container_name: hive-platform-worker
              command: celery -A app.scheduler.celery_app worker --loglevel=info --concurrency=4
              env_file: [ backend/.env ]
              environment: [ "SENTRY_ENVIRONMENT=staging" ]
              depends_on: [ postgres, redis ]
              networks: [ hive-platform ]
            celery-beat:
              image: ghcr.io/${{ env.IMAGE_OWNER }}/hive-small-file-api:${{ env.VERSION }}
              container_name: hive-platform-beat
              command: celery -A app.scheduler.celery_app beat --loglevel=info
              env_file: [ backend/.env ]
              environment: [ "SENTRY_ENVIRONMENT=staging" ]
              depends_on: [ postgres, redis ]
              networks: [ hive-platform ]
            frontend:
              image: ghcr.io/${{ env.IMAGE_OWNER }}/hive-small-file-frontend:${{ env.VERSION }}
              container_name: hive-platform-frontend
              depends_on: [ api ]
              ports: [ "80:80" ]
              networks: [ hive-platform ]
          volumes: { postgres_data: {}, redis_data: {} }
          networks: { hive-platform: { driver: bridge } }
          EOF
          fi
          mkdir -p backend frontend
          if [ ! -f backend/.env ]; then
            cat > backend/.env <<'EOF'
          SERVER_HOST=0.0.0.0
          SERVER_PORT=8000
          RELOAD=false
          DATABASE_URL=postgresql://postgres:postgres@postgres:5432/hive_small_file_db
          REDIS_URL=redis://redis:6379/0
          CELERY_BROKER_URL=redis://redis:6379/1
          CELERY_RESULT_BACKEND=redis://redis:6379/2
          SECRET_KEY=change-me-in-staging
          ACCESS_TOKEN_EXPIRE_MINUTES=1440
          DEFAULT_HIVE_HOST=localhost
          DEFAULT_HIVE_PORT=10000
          DEFAULT_HDFS_URL=http://namenode:9870/webhdfs/v1
          SMALL_FILE_THRESHOLD=134217728
          SENTRY_DSN=
          SENTRY_ENVIRONMENT=staging
          AUTO_CREATE_SCHEMA=false
          DEMO_MODE=false
          EOF
          fi
          if [ ! -f frontend/.env ]; then
            echo 'VITE_API_BASE_URL=/api/v1' > frontend/.env
          fi
          ls -lh docker-compose.prod.yml backend/.env frontend/.env

      - name: Build and Deploy on Runner Yun
        shell: bash
        run: |
          echo "=== 直接在Runner yun上构建和部署 ==="

          # 停止旧服务
          docker stop hive-platform-api hive-platform-frontend hive-platform-worker hive-platform-beat hive-platform-db hive-platform-redis >/dev/null 2>&1 || true
          docker rm hive-platform-api hive-platform-frontend hive-platform-worker hive-platform-beat hive-platform-db hive-platform-redis >/dev/null 2>&1 || true

          # 启动数据库服务
          echo "启动PostgreSQL数据库..."
          docker run -d --name hive-platform-db \
            -e POSTGRES_DB=hive_small_file_db \
            -e POSTGRES_USER=postgres \
            -e POSTGRES_PASSWORD=postgres \
            -p 5432:5432 \
            -v hive_postgres_data:/var/lib/postgresql/data \
            --restart unless-stopped \
            docker.io/library/postgres:14

          echo "启动Redis缓存..."
          docker run -d --name hive-platform-redis \
            -p 6379:6379 \
            -v hive_redis_data:/data \
            --restart unless-stopped \
            docker.io/library/redis:6-alpine

          echo "等待数据库启动..."
          sleep 15

          # 构建API镜像
          echo "构建API服务..."
          cd ${{ github.workspace }}/backend
          docker build -t hive-api:local .

          # 构建前端镜像
          echo "构建前端服务..."
          cd ${{ github.workspace }}/frontend
          docker build -t hive-frontend:local .

          # 启动API服务
          echo "启动API服务..."
          docker run -d --name hive-platform-api \
            --env-file /opt/hive-platform/backend/.env \
            -e SENTRY_ENVIRONMENT=staging \
            -e AUTO_CREATE_SCHEMA=false \
            -e RELOAD=false \
            -p 8000:8000 \
            --restart unless-stopped \
            hive-api:local

          # 启动前端服务
          echo "启动前端服务..."
          docker run -d --name hive-platform-frontend \
            -p 80:80 \
            --restart unless-stopped \
            hive-frontend:local

          # 启动Worker服务
          echo "启动Celery Worker..."
          docker run -d --name hive-platform-worker \
            --env-file /opt/hive-platform/backend/.env \
            -e SENTRY_ENVIRONMENT=staging \
            --restart unless-stopped \
            hive-api:local \
            celery -A app.scheduler.celery_app worker --loglevel=info --concurrency=4

          # 启动Beat服务
          echo "启动Celery Beat..."
          docker run -d --name hive-platform-beat \
            --env-file /opt/hive-platform/backend/.env \
            -e SENTRY_ENVIRONMENT=staging \
            --restart unless-stopped \
            hive-api:local \
            celery -A app.scheduler.celery_app beat --loglevel=info

          echo "=== 部署完成！检查服务状态 ==="
          docker ps | grep hive-platform

          echo "=== 运行健康检查 ==="
          sleep 5
          curl -f http://localhost:8000/health || echo "API健康检查失败"
          curl -f http://localhost/health || echo "前端健康检查失败"

      - name: DB migration
        shell: bash
        run: |
          cd /opt/hive-platform
          sleep 10 || true
          docker compose -f docker-compose.prod.yml exec api alembic upgrade head || true

      - name: Health checks
        shell: bash
        run: |
          echo 'API:'; curl -s http://127.0.0.1:8000/health || true
          echo 'FE :'; curl -s http://127.0.0.1/health || true

