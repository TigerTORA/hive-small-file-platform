#!/bin/bash

# å®Œæ•´é¡¹ç›®è¿ç§»åˆ°192.168.0.105æœåŠ¡å™¨
# åŒ…å«çŽ¯å¢ƒå®‰è£…ã€ä»£ç ä¼ è¾“ã€é…ç½®è°ƒæ•´ã€åŽŸç”ŸHDFSé›†æˆ

set -e

SERVER_IP="192.168.0.105"
SERVER_USER="root"
SERVER_PASSWORD="!qaz2wsx3edC"
PROJECT_DIR="/opt/hive-platform"
LOCAL_PROJECT_DIR="/Users/luohu/new_project/hive-small-file-platform"

echo "ðŸš€ å¼€å§‹å®Œæ•´é¡¹ç›®è¿ç§»åˆ°HDFSæœåŠ¡å™¨..."
echo "ç›®æ ‡æœåŠ¡å™¨: $SERVER_IP"
echo "é¡¹ç›®ç›®å½•: $PROJECT_DIR"
echo "è¿ç§»æ—¶é—´: $(date)"

# æ£€æŸ¥æœ¬åœ°é¡¹ç›®ç›®å½•
if [ ! -d "$LOCAL_PROJECT_DIR" ]; then
    echo "âŒ æœ¬åœ°é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: $LOCAL_PROJECT_DIR"
    exit 1
fi

# 1. æµ‹è¯•SSHè¿žæŽ¥
echo "ðŸ”Œ æµ‹è¯•æœåŠ¡å™¨è¿žæŽ¥..."
if ! ping -c 2 $SERVER_IP > /dev/null 2>&1; then
    echo "âŒ æ— æ³•pingé€šæœåŠ¡å™¨ $SERVER_IP"
    exit 1
fi

echo "âœ… ç½‘ç»œè¿žæŽ¥æ­£å¸¸"

# 2. åˆ›å»ºé¡¹ç›®æ‰“åŒ…æ–‡ä»¶
echo "ðŸ“¦ åˆ›å»ºé¡¹ç›®æ‰“åŒ…..."
cd /Users/luohu/new_project

# åˆ›å»ºä¸´æ—¶ç›®å½•
TEMP_DIR="/tmp/hive-platform-migration-$(date +%Y%m%d_%H%M%S)"
mkdir -p "$TEMP_DIR"

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶ï¼ˆæŽ’é™¤ä¸å¿…è¦çš„æ–‡ä»¶ï¼‰
echo "å¤åˆ¶é¡¹ç›®æ–‡ä»¶..."
rsync -av --exclude='.git' \
    --exclude='node_modules' \
    --exclude='__pycache__' \
    --exclude='*.pyc' \
    --exclude='.pytest_cache' \
    --exclude='dist' \
    --exclude='build' \
    --exclude='*.log' \
    --exclude='.env' \
    --exclude='.venv' \
    --exclude='venv' \
    --exclude='.DS_Store' \
    --exclude='frontend/dist' \
    --exclude='frontend/node_modules' \
    --exclude='backend/hive_small_file_db.db' \
    hive-small-file-platform/ "$TEMP_DIR/"

# 3. åˆ›å»ºæœåŠ¡å™¨çŽ¯å¢ƒå®‰è£…è„šæœ¬
cat > "$TEMP_DIR/setup_server_env.sh" << 'EOF'
#!/bin/bash
set -e

echo "ðŸ”§ é…ç½®æœåŠ¡å™¨å¼€å‘çŽ¯å¢ƒ..."

# æ›´æ–°ç³»ç»Ÿ
yum update -y
yum groupinstall -y "Development Tools"
yum install -y wget curl git vim htop tree unzip nc

# å®‰è£…Python 3.8+
yum install -y python38 python38-pip python38-devel
alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1
python3 -m pip install --upgrade pip

# å®‰è£…Node.js 18
curl -fsSL https://rpm.nodesource.com/setup_18.x | bash -
yum install -y nodejs

# å®‰è£…Claude Code CLI
npm install -g @anthropic/claude-code

# é…ç½®é˜²ç«å¢™
if systemctl is-active --quiet firewalld; then
    firewall-cmd --permanent --add-port=8000/tcp
    firewall-cmd --permanent --add-port=5173/tcp
    firewall-cmd --permanent --add-port=3000/tcp
    firewall-cmd --reload
fi

echo "âœ… æœåŠ¡å™¨çŽ¯å¢ƒé…ç½®å®Œæˆ"
EOF

# 4. åˆ›å»ºé¡¹ç›®é…ç½®è„šæœ¬
cat > "$TEMP_DIR/setup_project.sh" << 'EOF'
#!/bin/bash
set -e

cd /opt/hive-platform

echo "ðŸ“¦ å®‰è£…é¡¹ç›®ä¾èµ–..."

# åŽç«¯ä¾èµ–
cd backend
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn[standard] sqlalchemy pymysql
pip install requests python-multipart python-jose[cryptography]
pip install alembic celery redis pydantic

# å‰ç«¯ä¾èµ–
cd ../frontend
npm install

echo "âœ… é¡¹ç›®ä¾èµ–å®‰è£…å®Œæˆ"
EOF

# 5. åˆ›å»ºåŽŸç”ŸHDFSé…ç½®è„šæœ¬
cat > "$TEMP_DIR/configure_native_hdfs.sh" << 'EOF'
#!/bin/bash
set -e

cd /opt/hive-platform

echo "ðŸ˜ é…ç½®åŽŸç”ŸHDFSè®¿é—®..."

# æµ‹è¯•HDFSå‘½ä»¤
if command -v hdfs &> /dev/null; then
    echo "âœ… HDFSå‘½ä»¤å¯ç”¨"
    hdfs version | head -3
    
    # æµ‹è¯•HDFSè®¿é—®
    echo "æµ‹è¯•HDFSæ ¹ç›®å½•è®¿é—®..."
    if hdfs dfs -ls / > /dev/null 2>&1; then
        echo "âœ… HDFSè®¿é—®æ­£å¸¸"
    else
        echo "âš ï¸  HDFSè®¿é—®å¯èƒ½éœ€è¦æƒé™é…ç½®ï¼Œå°è¯•ä½¿ç”¨hdfsç”¨æˆ·"
        sudo -u hdfs hdfs dfs -ls / || echo "âŒ HDFSè®¿é—®å¤±è´¥"
    fi
else
    echo "âŒ HDFSå‘½ä»¤ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥Hadoopå®‰è£…"
fi

# åˆ›å»ºå¯åŠ¨è„šæœ¬
cat > start_backend.sh << 'BACKEND_EOF'
#!/bin/bash
cd /opt/hive-platform/backend
source venv/bin/activate
export PYTHONPATH=/opt/hive-platform/backend:$PYTHONPATH
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
BACKEND_EOF

cat > start_frontend.sh << 'FRONTEND_EOF'
#!/bin/bash
cd /opt/hive-platform/frontend
npm run dev -- --host 0.0.0.0 --port 5173
FRONTEND_EOF

chmod +x start_backend.sh start_frontend.sh

echo "âœ… åŽŸç”ŸHDFSé…ç½®å®Œæˆ"
EOF

# 6. åˆ›å»ºæµ‹è¯•è„šæœ¬
cat > "$TEMP_DIR/test_real_data.sh" << 'EOF'
#!/bin/bash
set -e

echo "ðŸ§ª æµ‹è¯•çœŸå®žHDFSæ•°æ®èŽ·å–..."

# æµ‹è¯•åŽŸç”ŸHDFSæ‰«æå™¨
python3 << 'PYTHON_EOF'
import sys
sys.path.append('/opt/hive-platform/backend')

from app.monitor.native_hdfs_scanner import NativeHDFSScanner

print("ðŸ” æµ‹è¯•åŽŸç”ŸHDFSæ‰«æå™¨...")
scanner = NativeHDFSScanner()

# æµ‹è¯•è¿žæŽ¥
print("æµ‹è¯•HDFSè¿žæŽ¥...")
result = scanner.test_connection()
print(f"è¿žæŽ¥çŠ¶æ€: {result['status']}")
if result['status'] == 'success':
    print(f"Hadoopç‰ˆæœ¬: {result.get('hadoop_version', 'Unknown')}")
    print(f"æ ¹ç›®å½•æ ·æœ¬: {result.get('sample_root_dirs', [])}")
else:
    print(f"è¿žæŽ¥å¤±è´¥: {result.get('message', 'Unknown error')}")

# æµ‹è¯•è·¯å¾„æ‰«æ
test_paths = ['/user', '/tmp', '/']
for path in test_paths:
    print(f"\næ‰«æè·¯å¾„: {path}")
    scan_result = scanner.scan_path(path)
    if scan_result['status'] == 'success':
        print(f"  æ–‡ä»¶æ•°é‡: {scan_result['total_files']}")
        print(f"  å°æ–‡ä»¶æ•°: {scan_result['small_files_count']}")
        print(f"  æ€»å¤§å°: {scan_result['total_size']} bytes")
        print(f"  æ‰«ææ¨¡å¼: {scan_result['scan_mode']}")
    else:
        print(f"  æ‰«æå¤±è´¥: {scan_result.get('message', 'Unknown error')}")
    
    if scan_result['status'] == 'success':
        break  # æ‰¾åˆ°ä¸€ä¸ªå¯è®¿é—®çš„è·¯å¾„å°±åœæ­¢

print("\nâœ… HDFSæ‰«ææµ‹è¯•å®Œæˆ")
PYTHON_EOF

echo "ðŸŒ å¯åŠ¨æœåŠ¡è¿›è¡ŒWebæµ‹è¯•..."
echo "åŽç«¯: http://192.168.0.105:8000"
echo "å‰ç«¯: http://192.168.0.105:5173"
echo "APIæ–‡æ¡£: http://192.168.0.105:8000/docs"
EOF

# 7. è®¾ç½®è„šæœ¬æƒé™
chmod +x "$TEMP_DIR"/*.sh

# 8. åˆ›å»ºé¡¹ç›®åŽ‹ç¼©åŒ…
echo "ðŸ“ åˆ›å»ºé¡¹ç›®åŽ‹ç¼©åŒ…..."
cd "$TEMP_DIR"
tar -czf hive-platform-complete.tar.gz *

echo "âœ… é¡¹ç›®æ‰“åŒ…å®Œæˆ: $TEMP_DIR/hive-platform-complete.tar.gz"

# 9. ä¼ è¾“åˆ°æœåŠ¡å™¨
echo "ðŸ“¤ ä¼ è¾“é¡¹ç›®åˆ°æœåŠ¡å™¨..."

# ä½¿ç”¨scpä¼ è¾“ï¼ˆéœ€è¦æ‰‹åŠ¨è¾“å…¥å¯†ç ï¼‰
echo "è¯·è¾“å…¥æœåŠ¡å™¨å¯†ç : $SERVER_PASSWORD"
scp "$TEMP_DIR/hive-platform-complete.tar.gz" "$SERVER_USER@$SERVER_IP:/tmp/"

# 10. åœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œå®‰è£…
echo "âš™ï¸  åœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œå®‰è£…..."
echo "è¯·è¾“å…¥æœåŠ¡å™¨å¯†ç è¿›è¡ŒSSHè¿žæŽ¥..."

ssh "$SERVER_USER@$SERVER_IP" << 'REMOTE_COMMANDS'
set -e

echo "ðŸ“‚ è§£åŽ‹é¡¹ç›®æ–‡ä»¶..."
cd /tmp
tar -xzf hive-platform-complete.tar.gz
mkdir -p /opt/hive-platform
cp -r * /opt/hive-platform/
cd /opt/hive-platform

echo "ðŸ”§ æ‰§è¡ŒçŽ¯å¢ƒå®‰è£…..."
chmod +x *.sh
./setup_server_env.sh

echo "ðŸ“¦ å®‰è£…é¡¹ç›®ä¾èµ–..."
./setup_project.sh

echo "ðŸ˜ é…ç½®HDFSè®¿é—®..."
./configure_native_hdfs.sh

echo "ðŸ§ª æµ‹è¯•çœŸå®žæ•°æ®èŽ·å–..."
./test_real_data.sh

echo ""
echo "ðŸŽ‰ é¡¹ç›®è¿ç§»å®Œæˆï¼"
echo ""
echo "ðŸ“ é¡¹ç›®ä½ç½®: /opt/hive-platform"
echo "ðŸš€ å¯åŠ¨å‘½ä»¤:"
echo "  åŽç«¯: cd /opt/hive-platform && ./start_backend.sh"
echo "  å‰ç«¯: cd /opt/hive-platform && ./start_frontend.sh"
echo ""
echo "ðŸŒ è®¿é—®åœ°å€:"
echo "  åŽç«¯API: http://192.168.0.105:8000"
echo "  å‰ç«¯ç•Œé¢: http://192.168.0.105:5173"
echo "  APIæ–‡æ¡£: http://192.168.0.105:8000/docs"
echo ""
echo "ðŸ“ ä¸‹ä¸€æ­¥:"
echo "1. å¯åŠ¨åŽç«¯æœåŠ¡"
echo "2. å¯åŠ¨å‰ç«¯æœåŠ¡"
echo "3. æµ‹è¯•çœŸå®žHDFSæ•°æ®èŽ·å–"
echo "4. å¼€å§‹åœ¨æœåŠ¡å™¨ä¸Šå¼€å‘"

REMOTE_COMMANDS

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
echo "ðŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
rm -rf "$TEMP_DIR"

echo ""
echo "ðŸŽŠ å®Œæ•´è¿ç§»å®Œæˆï¼"
echo ""
echo "ðŸ”— SSHè¿žæŽ¥åˆ°æœåŠ¡å™¨å¼€å§‹å¼€å‘:"
echo "ssh $SERVER_USER@$SERVER_IP"
echo ""
echo "ðŸ’¡ å»ºè®®:"
echo "1. åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…VS Code Serveræˆ–é…ç½®è¿œç¨‹å¼€å‘çŽ¯å¢ƒ"
echo "2. é…ç½®Gitç”¨æˆ·ä¿¡æ¯"
echo "3. è®¾ç½®å¼€å‘å·¥å…·å’Œä»£ç ç¼–è¾‘å™¨"
echo ""