# Hive 外部表测试配置文件
# 使用方法: source test-table-config.conf && ./create_test_external_table.sh

# === 基础配置 ===
export TABLE_NAME="test_small_files_table"
export DATABASE_NAME="test_db"
export HDFS_BASE_PATH="/user/test/small_files_test"

# === 高级配置 ===
export PARTITION_COUNT=10           # 分区数量
export FILES_PER_PARTITION=100      # 每分区文件数
export FILE_SIZE_KB=50              # 单文件大小(KB)

# === 不同场景的预设配置 ===

# 轻量测试场景 (100个小文件)
# export PARTITION_COUNT=5
# export FILES_PER_PARTITION=20
# export FILE_SIZE_KB=30

# 中等测试场景 (500个小文件)
# export PARTITION_COUNT=10
# export FILES_PER_PARTITION=50
# export FILE_SIZE_KB=40

# 重度测试场景 (2000个小文件) - 警告：资源消耗大
# export PARTITION_COUNT=20
# export FILES_PER_PARTITION=100
# export FILE_SIZE_KB=60

# 超大测试场景 (10000个小文件) - 警告：仅在高配置测试环境使用
# export PARTITION_COUNT=100
# export FILES_PER_PARTITION=100
# export FILE_SIZE_KB=64

# === 环境变量检查 ===
# 如果需要，可以在这里设置 Hadoop/Hive 相关环境变量
# export HADOOP_HOME=/opt/hadoop
# export HIVE_HOME=/opt/hive
# export PATH=$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH

echo "配置加载完成:"
echo "  - 表名: $TABLE_NAME"
echo "  - 数据库: $DATABASE_NAME"
echo "  - 分区数: $PARTITION_COUNT"
echo "  - 每分区文件数: $FILES_PER_PARTITION"
echo "  - 总文件数: $((PARTITION_COUNT * FILES_PER_PARTITION))"
echo "  - HDFS路径: $HDFS_BASE_PATH"